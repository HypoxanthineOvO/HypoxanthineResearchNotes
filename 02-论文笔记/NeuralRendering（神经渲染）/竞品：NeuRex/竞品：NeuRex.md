---
tags:
  - NR
  - Co-Design
---

# 1. Abstract

本文介绍NeuRex，一种使用算法增强和支持硬件高效执行现代神经渲染流水线的加速器架构。 NeuRex利用对最先进神经场景表示分析的深入理解，使现代神经渲染中的关键操作原语——多分辨率哈希编码更加适应硬件，并具有专门的哈希编码引擎，使我们能够有效地执行原语和整个渲染流程。我们使用商用 28nm 工艺技术实现和综合 NeuRex，并在移动和高端计算平台上评估两个版本的 NeuRex（NeuRex-Edge、NeuRex-Server）在不同图像分辨率的场景中。我们的评估表明，与移动和高端消费者 GPU 相比，NeuRex 可实现高达 9.88 倍和 3.11 倍的加速，同时面积占用和能耗更低。

# 2. Introduction

神经渲染是一种新兴的方法，使用深度神经网络（DNN）以可控的方式合成照片逼真的图像或视频。通过将场景和物体编码在深度神经网络的权重中，神经渲染隐含地将输入坐标映射到一些数字值，如颜色或辐射度。与传统的显式 3D 表示（如 Polygonal Meshes, Voxels 或 Point Clouds）相比，隐式神经场景表示更加紧凑，能够捕捉复杂表面或形状的细节。

虽然神经渲染是计算机图形学中许多任务的有前途的方法，例如图像超分辨率和新视角合成，但实现高质量渲染需要大量计算。传统的神经渲染基于多层感知器（MLP）网络，它由一组全连接层组成。要渲染一幅图像，MLP需要查询数百万次，因为每个像素沿射线的每个样本点都需要通过神经网络运行，以产生对应于输入坐标的输出值。即使在高端消费者 GPU 上，这也使神经渲染过程极其缓慢。

因此，近年来有大量工作旨在通过算法增强来减少神经表示的训练和渲染时间。尽管计算机图形学界正在积极研究神经场景表示的重要性，但还没有系统评估当今硬件系统对工作负载的性能表现，并从硬件角度帮助理解其架构影响的研究。

在本文中，我们开始通过研究现代神经渲染算法的特点，并对几种典型模型进行深入地背景描述，以了解它们的架构影响以及计算和内存需求。特别是，我们对最先进的神经场景表示进行了详细的表征，该方法将注意力集中在使用多个哈希编码表的较小 MLP 上，每个哈希编码表包含可训练的特征向量（即输入编码参数），每个特征向量捕捉不同的网格分辨率，并且极大地减少了训练和渲染时间，同时也提高了渲染视图的质量。

尽管它在渲染时间和质量方面表现显着优于先前的作品，但我们观察到现有模型中使用的 **Multi-Resolution Hash Encoding** Primitive不利于硬件，并导致在通用计算平台上执行神经渲染流水线时面临多个挑战和低效。对商品 GPU 进行的分析结果显示，执行多分辨率哈希编码所需的时间比 MLP 计算更长，这两个操作是 **Serialized in execution** （串行执行）的。此外，由于 Hash Table 的不规则访问性质，The Large Encoding Table 需要适应 On-Chip Cache；否则，花费在编码查找上的时间会显着增加，同时整体训练和渲染时间也会增加。此外，每个 Hash Entries Access 访问仅使用来自一个 Cache line 或 Off-Chip Memory 的 64 Byte 数据中的 4 个 Byte，导致内存带宽的大量浪费。当执行输入编码和哈希表查找时，计算核心也未得到充分利用，因为 MLP 大小较小。简而言之，通用 GPU 是一种不平衡的设计点，并且在运行现代神经渲染模型方面效率低下。

本文介绍了 NeuRex，一种神经渲染加速器，通过算法增强和支持硬件的渲染流水线执行流程的变化，有效地执行现代神经图形计算。 NeuRex 根据我们深入分析得出的关键观察结果构建，并使多分辨率哈希编码更加适合硬件并具有专用的哈希编码引擎，使我们能够有效地执行原语和整体神经渲染流水线。

我们算法增强的核心思想是将输入坐标网格划分为几个子网格，每个子网格拥有大型哈希编码表的一部分。然后，我们安排输入坐标的处理方式，使得在移动到另一个子网格之前，我们首先完成处理所有分辨率下的一个子网格。这限制了哈希表访问连续条目的范围，从而允许硬件加速器一次只加载哈希表的一部分到片上存储器中；因此，硬件加速器不需要使用多兆字节的片上存储器来有效地执行多分辨率哈希编码原语。这也使得有机会打破输入编码和 MLP 计算的串行执行，并有效地利用支持硬件将这两个操作重叠，从而提高加速器的总体计算和内存资源利用率。

我们使用商用的28纳米工艺节点实现了NeuRex的硬件组件，并通过建立一个周期级模拟器来进行性能评估，该模拟器对NeuRex的架构进行建模，并在图形学中的一组流行任务和数据集上进行评估。我们的评估表明，与代表性的移动计算平台（Jetson Xavier NX；Volta GPU；12纳米）和高端消费者计算平台（RTX 3070；Ampere GPU；8纳米）相比，NeuRex的两个变体分别实现了9.88×和3.11×的加速比，在小面积预算的情况下，面积分别为3.14mm2和21.37mm2。总之，本文做出以下贡献：

- 据我们所知，这是第一篇全面分析现代神经场景表示在当今计算平台上性能瓶颈的工作，并确定性能低效的根本原因。
- 我们提出了一种算法增强技术，使多分辨率哈希编码更加友好于硬件，从而无需多兆字节的芯片内存就可以高效地执行此基本功能。
- 我们呈现了NeuRex，这是一种硬件加速器，通过最小程度地扩展现有的DNN加速器，能够有效地执行神经图形计算。它具有针对现代神经渲染需求而量身打造的专用哈希编码引擎。

# 3. Motivation

在本节中，我们首先解释了Instant-NGP中使用的参数编码和神经渲染流水线（第3.1节和第3.2节）。  
然后，我们确定了对整体渲染时间有贡献的关键操作（第3.3节），并讨论了在GPU上执行流程的观察和瓶颈（第3.4节）。  

## 3.1. Multi-Resolution Hash Encoding

![[02-论文笔记/NeuralRendering（神经渲染）/竞品：NeuRex/images/Untitled.png|Untitled.png]]

Instant-NGP 引入了一种称为 Multi-Resolution Hash Encoding 的新原语。[[竞品：NeuRex]] 显示了基于哈希表的输入编码如何将输入位置映射到编码特征。首先，对于样本点 $s$ ，我们找到围绕该点的 Voxel ，并通过索引哈希表获得每个体素顶点的 $𝐹$ 维特征向量。哈希索引使用 Equation 2 中的哈希函数计算。每个 $x_v, y_v, z_v$ 都对应于 Voxel Grid 的顶点坐标。 $P_1$ 和 $P_2$ 是唯一的大质数， $⊕$ 表示按位异或运算。

$$h(x_v, y_v, z_v) = (x_v\cdot 1)\bigoplus (y_v\cdot P_1)\bigoplus (z_v\cdot P_2)\mod T$$

然后，我们对八个 $𝐹$ 维特征向量进行线性插值，以获得分辨率级别为 $𝐿$ 的样本输入点的 $𝐹$ 维特征向量。我们重复这些步骤 $𝐿$ 次，每次使用不同的网格分辨率（即不同的哈希表），并将来自所有级别的 $𝐿$ 个特征向量连接起来，从而得到一个大小为 $𝐹×𝐿$ 的输入向量用于 MLP。

多分辨率哈希编码使用 $𝐿$ 作为分辨率级别的数量。基础（即最粗糙）网格分辨率设置为 $16$ ，因此在基础分辨率（即 $𝐿=0$ ）下有 $163$ 个 Voxel 。分辨率按常数因子缩放以获得更细的级别（例如 $𝐿=1, 2, 3$ 等），从而以立方方式增加体素的总数。每个分辨率级别分配给一个独立的哈希表，每个哈希表最多具有 $𝑇$ 个哈希条目。每个条目包含一个 $𝐹$ 维特征向量，因此多分辨率哈希编码的可训练参数总数为 $𝐿×𝑇×𝐹$ 。[[竞品：NeuRex]] 显示了 Instant NGP 中多分辨率哈希编码的默认参数。我们在后续章节中使用相同的值进行讨论。

![[02-论文笔记/NeuralRendering（神经渲染）/竞品：NeuRex/images/Untitled 1.png|Untitled 1.png]]

## 3.2. GPU Execution FLow

![[02-论文笔记/NeuralRendering（神经渲染）/竞品：NeuRex/images/Untitled 2.png|Untitled 2.png]]

[[竞品：NeuRex]] 展示了使用多分辨率哈希编码的渲染流程的高级执行流程。最初，有 $N_\text{point}$ 个 Input Position，它们经过 $16$ 个哈希表，并产生一个 $N_\text{point}\times 32$ 的 Input Feature Matrix 。The Density MLP以 Feature Matrix 为输入，生成一个 $N_\text{point}\times 16$ 的矩阵，然后与一个 $N_\text{point}\times 16$ 的编码方向矩阵连接起来。得到的 $N_\text{point}\times 32$ 矩阵被输入到颜色MLP中，为每个输入位置生成3D RGB值（即 $\mathbf c_i$ ）。输入位置的数量（ $N_\text{point}$ ）可以根据图像分辨率而有数十万到数千万不等，例如，一个FHD图像有两百万个像素。请注意，我们需要对每个 Input Position 的每个 Level 执行 $8$ 次编码查找，这将导致总体上进行大量的哈希表查找。

## 3.3. Latency Breakdown

[[竞品：NeuRex]] 将渲染时间分解为五个主要的操作：Hash Encoding（ENC，哈希编码）、Feature Computation（MLP，特征计算）、Ray Compaction（Compaction，光纤压缩）、Empty Space Skipping（ESS，空点跳跃）和 Early Ray Termination（ERT，光线提前终止）。在实验中，我们在一系列 GPU 上运行使用大型 Fox 数据集（1920×1080 FHD分辨率）的 Instant-NGP ，包括边缘设备（Jetson Xavier NX）。

![[02-论文笔记/NeuralRendering（神经渲染）/竞品：NeuRex/images/Untitled 3.png|Untitled 3.png]]

结果显示，ENC 和 MLP 是操作中的主要性能瓶颈。特征计算（MLP）所占的渲染时间不到总时间的一半，这与原始的 NeRF-based Model 有很大不同，在原始模型中，MLP计算占据了整体渲染时间的主导地位。同时，Hash Encoding（ENC，包括哈希表查找和插值）占据了渲染时间的 $40\%$ 以上。请注意，这种 ENC 操作在当代 DNN 加速器中并不适用。

## 3.4. Observations and Inefficiencies

我们进一步研究了 Multi-Resolution Hash Encoding Primitive，并得出了以下关键的观察结果。

### Observation I: Performance portability of multi-resolution hash encodings

尽管哈希表查找的时间复杂度是 $𝑂(1)$，但这不**是一种适合硬件的操作**。一个设计良好的哈希函数会输出看似随机的哈希索引，从而导致对哈希表的非规则访问。正如前面提到的，现代最先进的神经表示模型训练了 $16$ 个哈希表（以及MLP权重），每个哈希表的大小都在几兆字节级别（例如，在 Instant NGP 中有 16个 2MB 的哈希表）。因此，它们无法全部适应当今大多数移动设备或消费级GPU/加速器的 On-Chip Memory ，因此如果我们简单地执行哈希表操作，就会**频繁发生 Off-Chip Memory Access** 。此外，每次哈希条目访问仅使用 $64$ Byte 数据中的四个字节（ $𝐹=2$ ），导致**片外存储器带宽的大量浪费**。

对于芯片内存容量大于单个哈希表的高端消费级GPU，一种解决方案是**按层级将哈希表加载到芯片内存**中，并在进行 $N_\text{point}×2$ 部分输入特征矩阵的流式传递时遍历所有样本点，以获得对应层级（$𝐿$）的矩阵，然后再移动到下一个层级（$𝐿+1$），以避免昂贵的片外存储器访问（这是GPU上的操作流程）；请注意，即使在这种情况下，如图6所示，哈希编码仍占据了渲染时间的40%以上。然而，对于移动设备和低端/中端消费级GPU，训练好的模型无法高效运行，因为即使单个哈希表也无法适应小型芯片内缓存，从而导致频繁进行芯片外存储器访问。

### Observation II: Serialized execution of rendering pipeline

如前所述，花费渲染时间最多的两个主要操作是哈希编码（ENC）和特征计算（MLP）。在使用多分辨率哈希编码的执行流程中，尽管它们具有不同的计算和存储需求，但**这两个主要操作会串行执行**；**ENC对存储带宽要求较高，而MLP需要更多的计算资源。**

请注意，由于 $N_\text{point}×32$ 的输入特征矩阵是逐级（即按列）构建的，所以在完成最细级别（ $𝐿=15$ ）的哈希表查找之前，我们无法对其余的管线进行MLP计算。理想情况下，如果我们能够并行执行 ENC 和 MLP ，可以加快渲染管线的速度。NeuRex通过对执行流程进行算法优化和支持硬件来实现这两个操作的重叠执行，并更好地利用硬件资源。

### Observation III: Difference in access characteristics across different levels of hash tables.

![[02-论文笔记/NeuralRendering（神经渲染）/竞品：NeuRex/images/Untitled 4.png|Untitled 4.png]]

图7显示了粗级别（ $𝐿=1$ ）和细级别（ $𝐿=13$ ）表的哈希表访问分布情况。对于没有哈希冲突的分辨率级别（例如， $𝐿=1$ ），哈希表条目**仅被分配给体素网格的单个顶点**。此外，一个体素中有大量的采样位置共享相同的顶点。因此，访问在几个条目上有一定的局部化，并且每个条目的访问次数很高。另一方面，**对于更细的分辨率级别（例如，𝐿=13），访问更均匀（且随机）地分布在哈希表条目之间，而每个条目的访问次数非常低。**基于这一观察结果，NeuRex具备两种不同类型的专用片上存储器，以有效地处理粗级别和细级别的编码查找操作。

# 4. NeuRex: Neural Graphics Engine

## 4.1. Execution Flow in NeuRex

![[02-论文笔记/NeuralRendering（神经渲染）/竞品：NeuRex/images/Untitled 5.png|Untitled 5.png]]

[[竞品：NeuRex]] 展示了NeuRex中神经渲染管线的高级执行流程。NeuRex执行流程与原始流程的主要区别在于哈希编码（ENC）和MLP操作的流水线化和重叠执行。例如，正如之前讨论的那样，在原始流程中，这两个主要操作被串行化，ENC和MLP都对关键路径延迟有所贡献。然而，NeuRex打破了串行化，并通过以一批位置为粒度处理输入位置，使它们并行执行。例如，我们首先加载一批输入位置（𝐵），并为该批次执行多分辨率的哈希编码。我们逐级处理批次，以利用批内哈希条目的局部性。完成ENC后，我们获得一个 𝐵 × 32 的部分输入特征矩阵，可以将其输入MLP。然后，在上一批次通过全连接层时，我们获取新的批次并执行ENC操作。通过这样做，NeuRex更好地利用计算单元和内存带宽。

## 4.2. Restricted Hashing

我们提出了一种 Hardware-Friendly 的 Multi-Resolution Hash Encoding 方法，有效地实现了NeuRex执行流程。我们增强的关键思想是**将 Input Coordinate Grid 划分为若干 Subgrids ，每个子网格都拥有每个层级的大型哈希表的一部分。**然后，我们安排输入点的处理方式是，**在处理另一个子网格之前，先完成所有分辨率下的一个子网格的处理。**通过这种方式，我们有效地将顶点特征查找的哈希表访问限制在连续哈希条目的范围内，而不是随机分布在整个表中。

这给我们带来了两个关键好处。首先，它使得具有小型 On-Chip Memory 的加速设备（例如 Mobile / Edge 设备或低端GPU）能够避免昂贵的 Off-Chip Memory Access ，并通过每次只加载哈希表的一部分到片上存储器中，更高效地执行编码查找，从而使多分辨率哈希编码在多种计算平台上具备性能可移植性。其次，它为子网格内的一批输入提供了机会，可以与另一批MLP计算并行高效地进行哈希编码。

![[02-论文笔记/NeuralRendering（神经渲染）/竞品：NeuRex/images/Untitled 6.png|Untitled 6.png]]

[[竞品：NeuRex]] 展示了限制哈希机制，其中我们将属于同一子网格的输入位置聚集在一组批次中。通过这样做，从同一批次中访问哈希表的区域被限制在表的一个小子集上，我们称之为 Subtable 。例如，在图中，Subgrid 6中的样本位置只访问 Subtable 6 中的哈希条目，并使用新的哈希函数进行索引。当我们将3D场景（即三维网格）划分为 $𝑅$ 个维度的子区域时，子网格的数量变为 $R^3$，并且哈希表也被均等地划分为 $R^3$ 个子表。我们将 $𝑅$ 称为子网格分辨率（sugbrid_res）。然后，对于每个输入位置（ $p$ ），我们可以使用公式3计算输入属于哪个子网格（subgrid_id）。

$$\text{subgrid-id}=\sum_{k=0}^2\lfloor p_k\cdot\text{subgrid-res}\rfloor \cdot \text{subgrid-res}^k$$

其中 $p = (p0, p1, p2) = (x,y,z)$，并且 $x,y,z ∈ [0, 1]$ 。

通过使用批次的子网格索引，我们将相应的子表加载到 on-chip buffer 中。这使得我们可以仅从片上存储器中执行编码查找，而不需要进一步访问片外存储器。请注意，用于访问子表中条目的新哈希索引可以使用公式2进行计算，只需对取余运算进行微小修改（即使用子表大小替代表的大小）。

## 4.3. Architecture Overview

![[02-论文笔记/NeuralRendering（神经渲染）/竞品：NeuRex/images/Untitled 7.png|Untitled 7.png]]

[[竞品：NeuRex]] 展示了我们加速器设计的概述，它由两个主要模块组成：Encoding Engine（EE）和 Tensor Compute Engine（TCE）。TCE模块类似于传统的 DNN 加速器，采用类似TPU 的 Systolic Array 结构，并具有 memory buffers for data movement。NeuRex通过一个专用的硬件模块（EE）最小程度地扩展了现有的DNN加速器设计，该模块可以高效地执行多分辨率哈希编码。

在高层次上，Encoding Engine 负责执行 Hash Table Lookups 并插值查找获得的 Feature Vector 以生成 Input Feature Vector 。为此，首先将一批 Input Position 从片外存储器流式传输到位置 Buffer 中。每个 Batch 中的 Position 在处理下个 Batch 之前会针对所有 $𝐿$ 级别的 Encoding Lookups 进行处理。

编码引擎中的 Index Generation Unit（IGU）为每个输入位置生成相邻顶点的 Hash Index 和 Interpolation Weights 。使用哈希索引，编码查找单元（ELU）从片上缓冲区（Grid Cache或 Subgrid Buffer）获取已编码的顶点特征。之后，通过插值计算单元（ICU）计算最终的输入特征向量，并将其发送到TCE的输入缓冲区中。 TCE使用 systolic array 执行多层感知机（MLP）。由于权重大小较小且被大量重用，我们选择了类似TPU的权重固定数据流作为收缩阵列的数据流方式。接下来，我们详细描述编码引擎的关键硬件组件。

## 4.4. Index Generation Unit

索引生成单元（Index Generation Unit，IGU）由 $𝑁$ 个计算单元组成（我们的 NeuRex-Server/NeuRex-Edge 设计中为 64/8 个单元），可以并行进行计算。IGU由三个主要部分组成：位置缩放、哈希索引计算和插值权重计算。

首先，IGU 对输入坐标进行缩放，因为它在归一化为 0 到 1 之间的坐标范围的边界框内。根据分辨率级别（对应特定的分辨率），我们将输入缩放到目标级别的坐标系统。这可以通过简单的浮点乘法和加法运算来完成。缩放后，我们获得了两个有用的信息。坐标的整数部分表示网格索引，小数部分表示点相对于其体素内的相对位置。网格索引用于定位体素的相邻顶点进行哈希计算，而相对位置用于计算顶点的插值权重。

![[02-论文笔记/NeuralRendering（神经渲染）/竞品：NeuRex/images/Untitled 8.png|Untitled 8.png]]

给定网格索引（对应于体素的基本坐标），IGU通过将每个坐标值加一或加零来计算所有相邻顶点的坐标。[[竞品：NeuRex]]显示了如何在基本坐标为 $(3, 4, 2)$ 时获取七个相邻顶点的坐标。这些（包括基本）坐标是方程2中哈希函数的输入。IGU中的每个哈希索引计算单元负责并行计算顶点的哈希索引。在我们的设计中，计算单元被完全流水化，IGU每个周期产生 $𝑁×8$ 个哈希索引。

为了聚集顶点的特征，我们需要计算每个顶点的插值权重。这个权重是根据输入位置与体素内每个顶点之间的距离确定的，如图11(b)所示。需要注意的是，距离样本位置更近的顶点被赋予较大的权重，这意味着该顶点的特征向量更能代表该位置。方程式4展示了如何计算插值权重。IGU中的插值权重计算单元通过乘法器和减法器执行此操作，并且每个周期IGU还会产生 $𝑁 × 8$ 个权重。需要注意的是，由于篇幅原因，图10中没有显示哈希索引计算单元和插值权重计算单元，其中 $(x_v, y_v, z_v)$ 和 $(x_s, y_s, z_s)$ 分别是顶点和缩放后的位置坐标。

## 4.5. Encoding Lookup Unit

编码查找单元（ELU）负责执行顶点点的哈希表查找。需要注意的是，对于一个样本输入点，我们会获取 $8×𝐿$ 个哈希表条目。如我们在3.4节中所讨论的，哈希表的访问特征在不同的分辨率级别之间是不同的。我们可以将分辨率级别分为粗糙级别和细化级别两类。对于粗糙级别，访问显示出对一小部分哈希条目的高局部性。相反，对于细化级别，访问均匀分布在哈希表的条目上。基于这种观察，我们以不同的方式处理两类哈希表查找。对于粗糙级别，我们使用网格缓存，而对于细化级别，我们使用子网格缓冲区。

### Grid Cache

对于粗糙的分辨率级别（例如， $𝐿=0,1,2,...$ ），包含在同一个体素中的输入位置数量足够大。同时，对于一个输入位置的哈希表访问的粒度不是单个哈希条目，而是体素的八个顶点的一组条目。我们通过将这八个条目合并为一个包含有关体素网格的额外信息的数据块来利用这个观察结果，例如级别索引（Level-ID）和网格索引（Grid-ID）。然后，对于一个输入位置，我们使用其 gid 进行单次访问来获取合并的八个顶点特征。

图12显示了网格缓存（GC）的结构。GC由具有独立地址解码逻辑的大量分行SRAM组成，以维持高片上内存带宽。GC中的每个数据块包含一个体素的八个顶点的特征向量（8×4B=32B）。由IGU计算得到的gid用于索引GC。标签包含四个字段：一个1位有效位、gid的18位最高有效位、一个4位级别索引lid和一个3位计数器。GC是一种直接映射的缓存样式缓冲区，gid的低位用来索引银行和数据块。

请注意，如果GC不包含对gid请求的顶点特征，则会将八个顶点条目的内存请求发送到片外内存，并在请求缓冲区中记录请求地址和元数据。当数据逐个从片外内存返回时，我们在请求缓冲区中找到匹配的地址，并填充数据块条目同时增加计数器。请注意，它生成多个64B的请求，而我们每次只取返回的数据中的4B。只有当所有八个条目都从片外内存填充完毕时，数据块才变为有效状态。

### Subgrid Buffer

对于细分级别（例如 $𝐿=...，13,14,15$），我们将每个分区的哈希表加载到子网格缓冲区用于编码查找。请注意，子网格缓冲区包含处理子网格中分辨率级别的输入位置所需的所有哈希条目；因此，直到我们进入下一个级别之前，没有进一步的片外内存访问，这与GC不同。与网格缓存一样，子网格缓冲区也采用了大量分行以保持高内存带宽（在我们的实现中有32个分行）。然而，与网格缓存不同的是，子网格缓冲区的每个分行为查找提供单个哈希条目（即4个字节）。因此，当任何一个顶点查找落到相同的分行时，会发生分行冲突。然而，在32个分行中，我们经验证明，由于哈希编码操作与NeuRex中的MLP计算重叠，整体渲染时间并不显著增加。我们在评估中使用来自第8级的子网格缓冲区，因为它显示了最佳的整体渲染时间。请注意，如果需要，NeuRex支持任意值。

## 4.6. Interpolation Compute Unit

一旦编码查找完成，我们将来自查找的顶点特征与相应的权重进行聚合。插值计算单元（ICU）在四个阶段执行此操作。第一阶段将八个特征乘以相应的权重。其他三个阶段由加法树消耗。ICU具有64/8个完全流水线化的计算单元（分别在 NeuRex-Server 和 NeuRex-Edge 中），它将聚合后的特征向量发送到TCE中的输入缓冲区。

## 4.7. Tensor Compute Engine

神经渲染中的MLP仅包含少数几个小的全连接层。另一方面，采样输入的数量比FC层的宽度大几个数量级。小的MLP权重和大的输入维度为层融合提供了巨大机会，正如其他研究所观察到的那样[3]。我们也采用了基于融合的MLP计算来设计加速器。给定一批输入特征，张量计算引擎（TCE）在一系列FC和激活层上工作，并生成最终输出，而无需将中间特征存储回片外内存。TCE具有足够大的输入和输出缓冲区来存储它们。

# 5. Experimental Methodology

## **Hardware Implementation**

我们使用 SystemVerilog 在 RTL 中实现了 NeuRex 的硬件组件。通过对合成数据进行RTL仿真，验证了每个组件的功能。我们使用 Synopsys Design Compiler 在商业28nm技术节点上进行了NeuRex组件的综合。On-chip SRAM也是使用相同技术的商业内存编译器生成的。EE 中的位置/子网格缓冲区和TCE中的输入/输出缓冲区都是双缓冲的。为了考虑未来每级表大小显著增加的情况，子网格缓冲区大小为128KB，具有32个bank，但在我们的评估中也可以调整为32KB。网格缓存大小为64KB，具有32个bank，请求缓冲区可以处理最多64个地址和每个地址64个合并请求。我们的架构设计以1GHz的时钟频率运行，除了内部运行双倍泵浦时钟为2GHz的片上存储器，以提供高片上存储带宽，与[50]类似。

为了评估使用片外存储器的NeuRex的系统级性能，我们还使用 Ramulator 实现了一个周期级模拟器，用于模拟NeuRex架构的DRAM时序。我们通过在GPU上运行工作负载来收集位置跟踪，并将其用作模拟器的输入。模拟器的时序参数基于RTL综合结果确定。我们使用模拟器报告的周期来衡量加速器的性能。模拟器还输出SRAM访问次数，我们使用这些数据来获取片上缓冲区的能耗。片外存储器的能耗使用来自内存模拟器的DRAM统计数据计算而得。

我们评估了NeuRex的两个变体：NeuRex-Edge和NeuRex-Server。NeuRex-Edge是在严格的面积和功耗约束下设计的，这是移动和边缘计算平台的典型情况。NeuRex-Server是为高端计算平台而扩大规模的架构。NeuRex-Edge的批处理大小设置为1024和8192。我们将NeuRex-Edge的片外存储配置为LPDDR4-3200，并使用DRAMPower进行统计分析。NeuRex-Server采用HBM2，并使用来自FGDRAM的能量模型。我们使用32×32的 systolic array 的倍数，而不是更大的一个；这提高了计算单元的利用率。NeuRex-Edge的TCE由一个32×32的systolic array组成，而NeuRex-Server有十六个32×32的systolic array。第6.5节讨论了两个NeuRex变体的硬件配置和能量效率。

## **Baselines**

我们将我们的加速器设计与两种不同类别的计算平台进行比较。我们选择 NVIDIA Jetson Xavier NX 作为 Edge Device 的代表。同时，选择 RTX 3070 作为高端消费级渲染加速硬件。我们使用并修改了作者发布的代码，其中包括经过大量优化的CUDA内核（例如，融合的MLP和其他优化，以更好地利用张量核心）。我们通过使用内置的硬件计数器来测量每个GPU的性能和功耗。请注意，RTX 3070采用了三星8nm工艺节点制造，这比NeuRex使用的技术节点（28nm）先进了几代。

## Workloads

![[02-论文笔记/NeuralRendering（神经渲染）/竞品：NeuRex/images/Untitled 9.png|Untitled 9.png]]

[[竞品：NeuRex]] 显示了我们用来评估我们设计的工作负载。我们精选了一系列合成和真实世界数据集，这些数据集来自于几个先前的研究，涵盖了分辨率和复杂性各异的场景。初始射线的数量与渲染图像的分辨率成比例，而射线采样迭代次数取决于场景的逼真程度。除了NeRF，我们还使用其他图形任务来评估我们的设计，例如神经符号距离函数（SDF）和2D图像近似，以展示基于参数化编码的神经场景表示方法在第6.6节中的通用适用性。

# 6. Evaluation

## 6.1. NeuRex Performance

![[Untitled 10.png]]

[[竞品：NeuRex]] 显示了NeuRex在RTX 3070和Xavier NX上的性能。平均而言，NeuRex-Server和NeuRex-Edge相对于基准GPU实现了2.88倍和9.17倍的加速。值得注意的是，与NeuRex-Server相比，NeuRex-Edge对基准的加速效果更高。这是因为当GPU具有较小的片上内存容量时（例如Xavier NX的256KB L2缓存），对大型编码表的不规则访问很快成为GPU执行的性能瓶颈。通过采用受限制的哈希和逐段加载编码表的方式，NeuRex提升了多分辨率哈希编码的性能可移植性。

![[Untitled 11.png]]

[[竞品：NeuRex]] 比较了NeuRex和GPU在现代神经渲染中的两个关键操作（哈希编码和特征计算）的性能。需要注意的是，图14(a)显示的加速来自受限制的哈希算法和专用片上内存设计。图14(b)显示，尽管与GPU相比，NeuRex的峰值计算吞吐量较低，但它执行MLP计算的速度更快。这是因为由于FC层较小，GPU张量核心的利用率较低，而NeuRex中的TCE实现了更高的计算利用率。此外，NeuRex的整体加速度（图13）高于对ENC和MLP的单独加速度，因为这两个操作在原始执行流中是串行的，而NeuRex通过受限制的哈希使它们可以重叠执行。

## Rendering Quality

受限制的哈希编码对多分辨率哈希编码进行轻微修改，以使其适应硬件。为了证明我们提出的方案不会降低渲染图像的质量，我们比较了原始哈希编码原语（Org-Hash）和受限制哈希编码（Ours）之间的平均峰值信噪比（PSNR）。对于每个场景，我们从10个不同的摄像机视角获得PSNR值，并取平均值以得出结果。

![[Untitled 12.png]]

[[竞品：NeuRex]] 显示，在应用于 Instant NGP 中的默认表大小（Ours-DT；每个级别2MB）时，受限制的哈希编码与基准相比，PSNR仅下降了可忽略的范围（0.7%～3.9%）。需要注意的是，Ours-DT已经优于原始的不使用哈希编码的基于NeRF的模型。如第4节所讨论的，受限制的哈希编码限制了每个 Batch 只能访问单个子网格缓冲区内的输入编码。因此，增加哈希表大小对性能影响较小，因为每次只需加载部分表项到芯片上。基于这一观察结果，我们配置了一个4倍大的哈希表（Ours-LT；每个级别8MB），以进一步提高PSNR而不影响性能。结果显示，Ours-LT在最坏情况下仅导致PSNR轻微下降1.1%，在其他几个场景中，甚至产生比Org-Hash更高的PSNR值。

![[Untitled 13.png]]

[[竞品：NeuRex]] 将原始哈希编码（Org-Hash）和受限制哈希编码（Ours）用于PSNR下降最大的场景，将参考图像与渲染图像进行了比较。我们可以看到，Ours-DT/LT并没有降低渲染质量，并且有趣的是，尽管PSNR较低，但使用受限制哈希编码生成的图像在某些部分与参考图像更接近于Org-Hash。这可能是因为一些部分比Org-Hash中单个哈希表的情况下发生更少的哈希冲突。需要注意的是，离芯片存储器足够大，可以容纳增加的哈希表，这使得受限制哈希编码成为边缘和移动平台上有吸引力的解决方案。

## 6.3. Source of Performance Gain

![[Untitled 14.png]]

  

[[竞品：NeuRex]] 展示了NeuRex中每个组件的加速比。我们可以将 NeuRex 分为两个关键组件：Grid Cache （GC）和 Restricted hashing with the subgrid buffer（RH）。通过累积 Baseline 上的每个优化，我们评估了 NeuRex 的三个变体：Baseline、GC 和 GC+RH 。Baseline 是没有进行任何优化的Batch-based Execution Model。需要注意的是，GC 和 RH 分别用于编码查找粗级别和细级别的内容。级别阈值设置为8。同时，对于不使用 GC 或 RH 的变体，我们使用 2MB 的缓存来模拟GPU中的传统缓存，这与单个哈希表的大小相同。所有的配置都适用于 NeuRex-Server。

GC相对于基准线的加速比来自于在粗级别上比传统缓存更有效地利用芯片内存带宽。在基准线中，我们需要访问缓存八次才能获取单个采样点的所有顶点特征。此外，每次访问只使用64B缓存行中的4B，导致芯片内带宽的浪费。相比之下，网格缓存可以在一次访问中提供合并的八个顶点特征，因此可以有效地为粗级别的哈希编码查找提供小缓存容量。通过在子网格缓冲区中最大化哈希条目的重用，NeuRex（GC+RH）在基准线上进一步提高了性能。

![[Untitled 15.png]]

## 6.4. Sensitivity Study

本节评估了NeuRex在不同硬件资源配置下的性能变化。特别地，我们关注两个配置：批处理大小和网格缓存大小。图18展示了在不同批处理大小和网格缓存大小下与GPU相比的加速比。我们只展示了使用Fox数据集的NeuRex-Server的结果，因为整体趋势是一致的。

### Batch Size

Batch Size 会影响性能，因为较大的批处理可以增加网格缓存的时间局部性，并提高TCE的计算利用率。然而，较大的批处理会导致一些芯片上缓冲区（如位置缓冲区）的大小增加。我们将批处理大小从2048增加到32768，并观察到在达到8192后，速度提升并没有显著增加。这是因为从片外内存中填充双缓冲子网格缓冲区的流式传输延迟在很大程度上被用于精细级别编码查找的芯片上内存访问延迟所隐藏。对于NeuRex-Server和NeuRex-Edge，我们选择了 Batch Size 为 $8192$ 和 $1024$ ，以平衡性能和资源开销。

### Grid Cache Size

我们观察到一个小的 Grid Cache Size就足以实现网格缓存的全部优势。我们对网格缓存大小进行了从16KB到256KB的扫描，并发现速度的提升在64KB之前显著增加，但对于更大的缓存并没有太大影响。网格缓存的大小与粗略级别中唯一体素的数量密切相关，因为每个缓存行都有一个体素的八个顶点特征。由于大多数输入批次属于粗略级别中少于2048个唯一体素，我们可以用一个小的网格缓存覆盖大部分体素。在我们的设计中，我们使用了一个64KB的网格缓存。

## Area amd Energy Efficiency

![[Untitled 16.png]]

表4显示了NeuRex的面积和功耗数据，表明NeuRex可以实现小面积和低功耗。需要注意的是，我们实现了类似TPU的TCE，但也可以选择其他设计。对于NeuRex的关键组件编码引擎而言，最大的面积开销是由于包含多个FP和整型MAC单元的IGU。然而，与基准的GPU SoCs（例如RTX 3070的392mm2和Xavier NX的350mm2）相比，NeuRex-Server和NeuRex-Edge的面积都可以忽略不计。

![[Untitled 17.png]]

图19展示了NeuRex和GPU之间的能量比较，其中NeuRex显示出比RTX 3070和Xavier NX更高的能效。需要注意的是，NeuRex采用28纳米技术节点制造，而GPU则采用更先进的节点（RTX 3070/Xavier NX为8纳米/12纳米）。因此，不应直接比较这些数据，更合适的推断是如果NeuRex采用更先进的技术进行制造，其吸引力将会更大。

## 6.6. Discussion

### **Restricted Hashing and Pipelining on GPUs.**

虽然受限散列（RH）和软件流水线（PP）可以应用于GPU，但我们观察到GPU并不能从中获得很大的好处。在图20中，RH表示我们仅将受限散列应用于GPU的情况。对于Xavier NX，受限散列有助于减少离片外存访问，因为我们只加载部分哈希表以在芯片上处理子网格。然而，现在我们将多个较小的输入矩阵馈送到MLP进行所有采样点的特征计算，而不是一个单独的大矩阵。这实际上增加了特征计算的总执行时间，因为每个MLP的核心利用率更低。总体而言，受限散列改善了性能，但增幅有限。RTX 3070已经具有较大的L2缓存，因此受限散列并不能帮助减少离片外存访问。此外，由于边缘GPU的相同原因，核心利用率比没有受限散列的情况更低。因此，实际上，除了Fox数据集之外，受限散列会降低大多数场景的性能。

RH+PP指的是在受限散列之上还应用了软件流水线的情况。虽然CUDA现在支持并发内核执行，但我们观察到，在复杂且经过优化的内核的执行中要实现良好的重叠是有挑战性的，正如之前的研究所指出的[60]，这与将内核执行与数据传输（例如cudaMemcpyAsync）重叠的情况不同。这是因为硬件资源（例如寄存器、共享内存）是有限的，而CUDA运行时对于多个复杂内核的线程块的调度并不高效；用户对此具有有限的控制权。我们观察到，只有哈希编码和MLP内核之间的执行的一小部分可以重叠。另一方面，启用重叠所需的开销（例如同步）较高，因此RH+PP甚至与RH相比降低了性能。我们还观察到，通过减少每个内核的资源使用量，可以增加重叠的部分，但这会导致每个内核的执行时间增加。

### Long-Term Viability of NeuRex

正如第2.3节所讨论的，Instant-NGP并没有显著改变NeRF模型的架构，而是通过采用多个可学习的哈希表对输入位置进行编码的简单改变。这种输入编码方法在某种程度上类似于Transformer-based模型中使用的位置编码和词嵌入，这已经成为自然语言处理（NLP）中必不可少的元素[7, 14, 56]。类似地，哈希编码原语可以应用于其他图形工作负载或任务，比如神经有符号距离函数（SDF）[46]和图像近似（Gigapixel）。图21显示，NeuRex还可以改善GPU上SDF和图像近似的性能。我们预见到这种新的输入编码技术将在未来得到广泛应用，每当适用时，NeuRex都可以帮助提高这些工作负载的性能，超越NeRF。