# Abstract

神经场景表示（NSR）是通过从不同相机位置拍摄的数十张照片中学习，用神经网络对三维场景进行编码的一种新方法。NSR 不仅能显著提高新颖视图合成和三维重建的质量，还能降低相机成本，从昂贵的激光相机降低到廉价的彩色相机。然而，由于硬件利用率极低（仅为硬件峰值性能的 0.22%），使用 NSR 进行三维场景编码远非实时，这极大地限制了其在实时 AR/VR 交互中的应用。

在本文中，我们提出了用于实时 NSR 学习的完全融合片上处理架构 Cambricon-R 。最初，通过对 GPU 上 NSR 的计算模型进行深入分析，我们发现极低的硬件利用率主要是由碎片化阶段和大量不规则内存访问造成的。为了解决这些问题，我们提出了 Cambricon-R 架构，该架构采用了新颖的 Fully-Fused Ray-Based Execution Model，消除了实时 NSR 学习中计算和内存效率低下的问题。具体来说，Cambricon-R 采用 Ray-level fused architecture，不仅消除了 intermediate memory traffics，还利用场景中的点稀疏性消除了不必要的计算。此外，还提出了基于自动插值库阵列（AIBA）的高吞吐量片上内存系统，以高效处理大量不规则内存访问。

我们在 12 个常用数据集上对 Cambricon-R 进行了评估。我们在 12 个常用数据集上评估了 Cambricon-R 与最先进的代表算法 Instant-ngp。结果表明，Cambricon-R 的 PE 利用率平均达到 67.3%、 平均达到 67.3%。与在 A100 相比，Cambricon-R 的速度提高了 373.8 倍，平均节能 256.6 倍。平均节能 256.6 倍。更重要的是，它实现了实时 NSR 学习，平均每秒可学习 69.0 个场景。

# 1. Introduction

神经场景表示（NSR）和神经渲染是最近出现的一种有前途的技术，可重建逼真复杂的三维场景，其质量令人印象深刻。与传统的图形基元和渲染方法相比，NSR 技术省去了构建精确 3D 网格模型所需的大量人力，并降低了相机成本，从昂贵的激光相机到廉价的彩色相机。因此，通过自动重建和渲染逼真的三维场景，NSR 可以提高增强现实（AR）和虚拟现实（VR）应用的生产率。

![[02-论文笔记/NeuralRendering（神经渲染）/竞品：Cambricon-R/images/Untitled.png|Untitled.png]]

[[竞品：Cambricon-R]] 显示了 NSR 技术的两个阶段。首先，在学习阶段，NSR 通过学习从不同空间位置拍摄的多张照片，将三维场景隐式编码为可训练参数。其次，在推理阶段，学习到的参数可用于实现许多应用，如新颖的视图渲染、风格化、重新照明，以及对这些参数编码的三维场景进行结构编辑。在本文中，我们将重点放在加速 NSR 算法的学习阶段，因为学习 NSR 模型会产生巨大的计算开销，这是限制 NSR 技术广泛应用的关键因素。如[[竞品：Cambricon-R]] 所示，学习过程以几张二维照片为输入，在以下阶段执行数百次前向-后向-更新迭代。

![[02-论文笔记/NeuralRendering（神经渲染）/竞品：Cambricon-R/images/Untitled 1.png|Untitled 1.png]]

1. 点采样（Point Sampling）是沿着三维场景范围内的一条射线对一组点进行采样（每条射线从相机点出发，穿过输入照片的一个像素）。
2. 点编码（Point Encoding）将每个采样点的坐标和视图方向编码为特征向量。
3. MLP 将编码后的特征向量作为输入，然后执行 MLP 网络来预测每个采样点的发射颜色和体积密度。
4. Loss通过沿每个图像像素对应的射线进行射线积分，累积采样点的颜色和密度，然后计算预测像素与地面实况像素之间的损失。
5. 参数更新 使用从损失中学习到的梯度更新所有可训练参数。

在现有的主流 GPGPU 平台上，NSR 学习算法的性能较低，这阻碍了其广泛应用。对于需要实时交互的 AR/VR 应用，SOTA 软件 NSR 学习实现与实时性要求之间存在着 321 倍的性能差距。我们注意到，NSR 学习算法在 GPGPU 上的硬件利用率极低。例如，最先进的 NSR 学习算法 instant-ngp 在 A100 GPU 上仅达到 0.22% 的硬件利用率 。

> [!important] 硬件利用率=
> 
> $𝑁𝑉÷𝑅𝑇÷𝑃𝑃$。$𝑁𝑉$ 表示计算有效点的操作次数，$𝑅𝑇$ 表示运行时间，$𝑃𝑃$ 表示设备的峰值性能（A100 GPU 为 312 Tflops ）。

原因有两个方面：

## Fragmentary stages

与传统神经网络相比，NSR 内核更加零散，计算与内存流量比非常低。在优化传统 DNN 模型时，通常会采用批处理策略来提高硬件利用率，但这种策略对零散阶段毫无帮助。与常识相悖的是，增加批处理大小不仅对数据重用没有什么帮助，甚至会在两个相邻内核之间产生大量中间数据，从而大大增加片外内存流量，并降低下一步的带宽效率。例如，单次迭代训练 NSR 的中间数据访问总量达到 1.05 GB，而内存占用仅为 570 MB 左右。因此，设计一种新的计算模型来提高 NSR 的计算效率至关重要。

## **The huge volume of fine-grained irregular memory accesses**

对于现有的 GPU 平台来说，编码阶段的大量细粒度不规则内存访问是一个具有挑战性的问题。因此，编码阶段的成本占训练迭代总开销的 29% 以上（图 3），并且存在巨大的片外内存访问（图 4）。细粒度子阵列架构可以缓解这一问题，但会带来巨大的库冲突和面积开销。这促使我们设计一种片上存储器系统，以支持高吞吐量的细粒度存储器访问。

为了解决这些问题，我们提出了 Cambricon-R 架构，该架构采用新颖的基于射线的全融合执行模型，消除了实时 NSR 学习中计算和内存效率低下的问题。新颖的基于射线的执行模型将所有点的计算阶段完全融合在一条射线中，瞄准了高计算并行性和内存效率的甜蜜点。Cambricon-R 同时处理多条射线以获得高性能，并消除了一条射线内的中间内存访问。Cambricon-R 还利用射线行进中的早期终止来消除不必要的计算，因为背景点被前景点遮挡，在 NSR 学习过程中可以放弃。此外，Cambricon-R 还包括一个基于自动插值库阵列（AIBA）的高吞吐量片上内存系统，以有效处理大量不规则内存访问。实验结果表明，Cambricon-R 实现了 67.3% 的 PE 利用率，与 A100 GPU 上最先进的解决方案相比，Cambricon-R 实现了高达 373.8 倍的提速和 256.6 倍的节能。本文的贡献总结如下。

- 我们对神经场景表示算法进行了详细分析，发现关键问题在于零碎阶段和不规则内存访问导致的硬件利用率低。
- 我们提出了基于射线的执行模型和硬件流水线，以提高计算并行性，并消除大部分片外内存访问，从而提高硬件利用率。按照基于射线的执行流程，我们实现了基于 GPU 的软件，其速度比最先进的解决方案快 1.398 倍。
- 我们提出的 Cambricon-R 是首个完全融合的片上处理加速器，其关键组件包括基于射线的融合计算单元和基于 AIBA 的片上内存系统，消除了内存效率低下的问题，并将 NSR 学习推向实时水平。
- 我们对 Cambricon-R 进行了全面评估，结果表明，与基线解决方案相比，Cambricon-R 的速度提高了 373.8 倍，能耗降低了 256.6 倍。

# **2. NSR Basis**

略过了，介绍算法的

# **3. NSR Characterization**

在本节中，我们通过在 A100 GPU 上使用 tiny-cuda-nn 框架运行 instant-ngp 算法，对 NSR 的性能进行了深入分析。尽管这已经是目前最先进的加速 NSR 训练的解决方案，但与实时应用的要求仍有 321 倍的性能差距。一般来说，要实现实时性，需要在一秒钟内对 60 个场景进行 NSR 学习，这意味着在 16.7 毫秒内完成一个场景的学习。然而，最先进的 NSR 学习算法至少需要 5 秒钟。分析结果表明，A100 GPU 执行 NSR 学习的计算效率仅为 0.22%。我们进行了深入分析，将硬件利用率低的原因归结为碎片化阶段和大量细粒度不规则编码数据访问，具体如下。

## **3.1 Fragmentary stages**

如[[竞品：Cambricon-R]] 所示，NSR 的学习过程包括五个不同的计算阶段：采样、编码、MLP、损失和更新。我们在 A100 上的两个典型数据集上细分了这五个阶段的运行时间，如图 3 所示。虽然 MLP 是计算最密集的阶段，但它只分别耗费了 fox 和 ficus 总运行时间的 17.33% 和 16.88%。相比之下，其他四个阶段在总运行时间中占主导地位，因为它们与 MLP 阶段完全不同，其中一些不适合 GPU 架构，导致计算效率低下。

![[02-论文笔记/NeuralRendering（神经渲染）/竞品：Cambricon-R/images/Untitled 2.png|Untitled 2.png]]

![[02-论文笔记/NeuralRendering（神经渲染）/竞品：Cambricon-R/images/Untitled 3.png|Untitled 3.png]]

从[[竞品：Cambricon-R]] 中可以发现， Loss Stage 占用了大量运行时间，而操作复杂度却远低于 Encoding 和 MLP 阶段。仔细查看 CUDA 内核的代码后，我们发现原因有二。首先，Loss Kernel 只启动了 32 个 Thread Block，这意味着它最多使用了 108 个 SM 中的 32 个 SM（流式多处理器）。其次，Warp 中的 32 个线程处理 32 条不同的射线。不同射线的数据相互独立。这导致线程在 Conditional Branches 和 Discontinuous Data Address 方面都出现了严重的分歧。因此，GPU 在执行 Loss Stage 时效率很低。

具有较大批量大小（通常为 262144）的 Fragmentary stage 还引入了大量的中间数据访问，进一步加剧了极低的计算效率。实际上，这些阶段是在不同的 CUDA Kernel 中实现的，因为它们需要不同的线程调度。例如，MLP Kernel 中的每个线程负责一系列样本点的张量的某个区块，而 Loss Kernel 中的每个线程则计算每条射线的射线积分。因此，将这些阶段编入一个融合内核是不切实际的。此外，由于引入大批次是为了提高计算并行性和 PE 利用率，相邻两个内核之间传输的中间数据的大小也会随着大批次的增加而相应增加。具体来说，训练 NSR 一次迭代的中间数据访问总量达到 1.05 GB，而内存占用仅约为 570 MB。这大大降低了计算效率。

## 3.2 Heavy fine-grained irregular memory accesses in the encoding stage

Encoding Stage 是性能瓶颈，因为需要进行大量细粒度的不规则内存访问。如图 3 所示，训练迭代总运行时间的 29% 以上用于 Encoding Stage，如[[竞品：Cambricon-R]] 所示，Off-chip Memory Access 总量的一半发生在这一阶段。 Encoding Stage 的内存访问具有挑战性，原因有三。

1. Fine-Grained Memory Access： Encoding Table 中的每个 Entry 通常由两个 FP16 Numbers 组成，比 GPU 中的 32-byte Cache line 短 8 倍；
2. Irregular Memory Access： 周围 Vertexes 的 Table Entry 是通过哈希函数计算的，这大大破坏了相邻顶点内存地址的 Locality；
3. Heavy Memory Access： 通常有 16 个不同网格分辨率的 Encoding Table ，每个表的 Entry Indexes 都是独立计算的。一个点需要访问周围顶点的 8 个 Entry 。因此，每个点需要 128 个 irregular addressed4-byte data fragments。

此外，在一次训练迭代中需要处理的点数量非常大。Batch Size 通常为 262,144 个，而在前向处理过程中，由于 GPU 没有使用提前终止功能，批量大小甚至扩大了 16 倍（详见第 4.3 节）。

尽管 instant-ngp 已通过高度优化的 NSR 框架 tiny-cuda-nn 进行了评估，但它在 Encoding 仍显不足。它为 Grid Encoding Kernel 实现了两个主要优化：低位精度和逐级执行流。低位精度通过使用 FP16 而不是 FP32 数据类型，将编码表数据量的一半减少到不超过 32 MB（包括编码数据及其梯度）。但这并不能解决哈希函数造成的内存访问顺序不规则的问题。逐级优化提高了 GPU 高速缓存的数据重用率。不过，根据英伟达官方工具（Nsight-cli 和 Nsight-Compute）的测量，在一次训练迭代中，缓存未命中率仍高达 15.5%，片外内存访问总量达 2.5 GB。原因有二。首先，有两类输入数据：编码表（最多 32 MB）和输入位置（最多 48 MB）。这些数据的总大小高达 80 MB，是 A100 GPU 二级缓存的两倍。其次，不规则的内存地址破坏了高速缓存的局部性。因此，大量细粒度的不规则内存访问成为 NSR 性能的瓶颈。

总之，Encoding Stage 的零碎阶段和大量细粒度的不规则内存访问导致现有 GPU 平台的计算效率极低，促使我们设计一种全新的加速器来实现实时 NSR 学习。

# **4. CAMBRICON-R**

在本节中，我们提出了用于加速 NSR 学习的完全融合的片上处理架构 Cambricon-R。首先，我们提出了基于射线的执行模型，而不是 CNN 图像分类中常用的基于像素的执行模型。基于射线的执行模型在很大程度上提高了中间数据的重用性和计算并行性。然后，我们设计了 Cambricon-R 加速器，为 NSR 学习实现完全融合的片上硬件流水线，其设计理念如下：

1. 为基于射线的片上执行模型提供硬件支持，消除 99% 的片外内存访问。
2. 设计早期终止流水线逻辑，在渲染过程中，当观察到背景像素被前景像素阻挡时，避免前向过程中不必要的计算。
3. 提出一种面积效率高的 NoC（Network-on-Chip）设计，解决了编码阶段大量细粒度不规则内存访问所导致的严重库冲突问题。

## **4.1 Ray-based execution flow**

GPU 采用基于阶段的执行流程来执行 NSR 学习。每个阶段都编入独立的 CUDA Kernel。所有内核均以 262144 个有效点的大批次规模执行。这就需要占用 571.1 MB 的大量内存。内存占用由两部分组成。一部分是可学习参数，包括 MLP 的权重（18 KB）和编码表中的条目（32 MB）。另一部分是临时数据，包括 MLP 中的训练像素、采样点、编码特征、激活度和梯度。所有点共享的可学习参数仅占总内存占用的 5.6%。剩余的内存空间用于临时数据，其大小与批量大小成正比。

这一观察结果启发我们降低数据处理的粒度，以减少内存占用。独立执行的最小粒度是一条射线上的训练，因为损失函数是使用一条射线上的所有有效点来计算的。根据这一思路，我们提出了基于射线的执行流程，一次处理一条射线的所有阶段。一条射线处理完毕后，这条射线的临时数据所占用的内存空间就可以释放，并分配给下一条射线开始处理。而更新阶段是在 NSR 一次迭代中最后一条射线处理完毕后执行的。因此，对于一条有 256 个点的射线来说，临时数据占用的内存不超过 512 KB。这对于片上 SRAM 实现来说是可以接受的大小。

## **4.2 Cambricon-R overall architecture**

Cambricon-R 的结构是根据基于射线的执行流程设计的，我们使用多核方案来扩大并行性，提高性能的上限。共有 128 个 MLP 单元，每个单元可以单独处理一条射线的 MLP 训练，包括 MLP 和损失阶段。为了节省片上内存空间，Cambricon-R 将 MLP 权重和编码表等可训练的共享参数放在全局缓冲区，将激活度和梯度等临时数据放在每个内核的局部缓冲区。此外，我们将 MLP 的权重缓存在本地缓冲区，以减少全局内存访问，因为 MLP 权重的大小只有 18 KB。下面我们将讨论 Cambricon-R 的设计和全面的数据流。

![[02-论文笔记/NeuralRendering（神经渲染）/竞品：Cambricon-R/images/Untitled 4.png|Untitled 4.png]]

### 4.2.1 Overall architecture and dataflow

[[竞品：Cambricon-R]] 显示了 Cambricon-R 最终设计的总体结构。它包含一个采样单元、一个编码单元、一个更新单元和 128 个 MLP 单元。MLP 单元包含两个收缩阵列、一个终止单元和一个损耗单元。损耗单元一次只处理一条射线，所有数据都位于本地内存中。根据我们的评估，由于采用了硬件流水线，损耗单元的运行时间全部由 MLP 运算覆盖。片上存储器包括一个全局缓冲区、编码单元中的大量编码缓冲区和 128 个本地缓冲区。全局缓冲区存储原始训练数据（像素 rgbs）、采样点、MLP 权重和二进制占格数据。编码缓冲区存储编码表及其梯度。本地缓冲区存储 MLP 的数据及其梯度、损失和累积权重梯度。

Cambricon-R 的数据流描述如下。在 NSR 训练开始时，一批原始训练数据从 DRAM 加载到全局缓冲区。同时，更新单元初始化权重，将其存储到全局缓冲区，然后广播给所有 MLP 单元。然后，采样单元执行采样过程，并将采样点写入全局缓冲区。然后，编码单元加载采样点，并将编码点输出到连接所有 MLP 单元的数据总线上。然后，MLP 单元开始对接收到的点进行前向处理，并在接收到的点的前向处理完成后，由终止单元估算一次提前终止。在终止单元返回一个有效的终止信号后，MLP 单元中的后向过程开始。后向过程产生两种数据。权重梯度被累积并存储在本地缓冲器中，而输入的梯度只存储在本地缓冲器中，但不会累积。当训练点总数超过训练批量大小后，这一过程将重复执行。

在每次训练迭代结束时，更新单元收集所有 MLP 单元的权重梯度，将其累积并更新全局缓冲区中的权重。同时，编码单元收集存储在所有 MLP 单元中的输入梯度，并更新编码表。上述训练迭代过程重复执行至少 250 次。每迭代 16 次后，在占用网格更新阶段，执行编码和 MLP 阶段的前向过程，计算整个空间中网格顶点的二进制密度。计算结果的二进制密度存储在全局缓冲区中。最后，将权重和编码表写入 DRAM，完成训练。

## **4.3 Early Termination Optimization**

![[02-论文笔记/NeuralRendering（神经渲染）/竞品：Cambricon-R/images/Untitled 5.png|Untitled 5.png]]

Instant-ngp 提出了提前终止机制，以避免对未用于训练的闭塞点进行计算。它根据每条射线中前景点的累积密度取消对背景点的计算。例如，对于一条总共有 256 个采样点的射线，如果前 41 个点的累积密度超过了阈值，那么剩下的 215 个点在 NSR 学习过程中就没有用了。然而，GPU 只能在后向过程中避免计算无效点，而在前向过程中浪费大量运算。原因如下。在 GPU 中，终止位置是在损失阶段计算的。由于逐级执行流程，GPU 必须在执行损失阶段之前完成所有采样点的编码和 MLP 阶段。这导致前向过程中编码和 MLP 阶段的计算浪费了 55.1%。

得益于细粒度的基于射线的执行流程，我们的设计可以在编码和 MLP 阶段的前进过程中实现提前终止。首先，我们将编码和 MLP 阶段的处理粒度从整条射线缩小到一组 32 个点。然后，我们在每个 MLP 单元中放置一个终止单元，并在计算完一组 32 个点后立即执行早期终止估计。[[竞品：Cambricon-R]] 显示了终止单元中有效执行终止估计的电路。因此，MLP 对无效点的前向计算浪费被限制在每条射线不超过 31 个点。

但是，由于 128 个 MLP 单元与全局编码单元之间的距离较远，取消无效点的编码成本比较困难。如果编码单元等到每个 MLP 单元返回终止信号后再开始编码下一组点，由于编码单元的延迟时间较长（详见下一小节），会导致 MLP 单元在等待下一组点时出现大量空闲周期。为了避免这些空闲周期，我们建议在不知道 MLP 单元提前终止信号的情况下立即执行下一组点的编码。如[[竞品：Cambricon-R]] 所示，返回编码单元的终止信号不会直接控制编码处理的开始或停止。它通过将新射线分配给相应的 MLP 单元来终止旧射线的编码。当 MLP 单元正在计算第 $i$ 组点的推理时，编码单元正在处理第 $(i+1)$ 组点。如果终止发生在第 $i$ 组，信号将返回编码单元，这样就不会处理第 $(i+2)$ 组点。第 $(i+1)$ 组点的编码将被浪费。但这种浪费不会使 MLP 单元停滞，因为在终止后，MLP 单元会开始计算 MLP 的损失和后退阶段。因此，这种解决方案能有效地最大化 MLP 单元的利用率。

### 4.3.1 GPU Optimization

![[02-论文笔记/NeuralRendering（神经渲染）/竞品：Cambricon-R/images/Untitled 6.png]]

基于射线的执行流程和提前终止的 GPU 实现。基于射线的执行流程和提前终止也可以通过基于 GPU 的软件方法来实现。我们对 tiny-cuda-nn（GPU 上最快的 NSR 学习框架）的 CUDA 内核进行了大量修改，将前向阶段的所有 CUDA 内核融合为一个大内核，包括采样、编码、MLP 和终止检测功能。

融合实现有两大优势。首先，它通过提前终止避免了无效点的计算。在优化程序中，融合内核执行前向阶段，并对射线中的每 16 个点进行迭代检测终止。其次，它有助于更好地并行利用不同类型的硬件资源。在优化后的程序中，不同区块的翘曲可以并行执行不同阶段，GPU 的翘曲调度器有助于充分利用闲置的硬件资源。相比之下，在基准程序中，每个 SM 上只执行一种类型的操作，因此所有经线都在等待相同类型的硬件资源。例如，编码阶段大量使用内存带宽，而张量内核则处于闲置状态。MLP 阶段则相反。优化后的程序避免了这种硬件资源的拥堵。

我们还尝试融合前向和后向阶段的内核，但发现对于 GPU 而言，只融合前向阶段的内核效率最高。我们发现，由于并行性较低，在每个区块中只处理一条射线的基于射线的原始执行流效率很低。然而，每个区块的射线数量过多会造成提前终止的浪费，并且会减少每个 SM 上执行的区块数量，从而无法很好地利用上述 warp 调度器的并行性。我们对设计空间进行了探索，发现如图 7 所示，每块 8 条射线是性能最高的最佳权衡方案。最后，优化后的程序与原始软件相比速度提高了 1.398 倍，成为基于 GPU 的最先进解决方案。

然而，经过优化的 GPU 与实时要求之间仍然存在巨大的性能差距。差距来自两个方面。首先，GPU 片上存储器系统与编码存储器访问模式不匹配。编码阶段要求片上存储器系统具备三个特点：

1. 存储所有编码表的大容量；
2. 所有 SM 的全局可见性；
3. 访问不规则 4 字节数据的高吞吐量。

具体来说，L1 Cache/Shared Memory 不符合前两个要求。L2 Cache 不符合第三项要求。实时 NSR 训练中的编码阶段要求每个周期对不规则寻址的 4 字节数据进行 4096 次访问（见第 4.4.1 节），而 A100 的二级缓存有 80 个片段，每个片段每个周期提供 64 字节数据。其次，由于缺乏专用的采样和损耗电路，这些轻量级功能的运行时间成本过高（见图 3）。这些都是软件方法无法解决的内在原因。

## **4.4 High Throughput Encoding Unit**

即使 Encoding Tables 存储在芯片上，对 Encoding Table 进行高效内存访问也是一项挑战。在本小节中，我们将介绍解决这一问题的设计目标、挑战、Baseline 和我们的设计。

### 4.4.1 Design goal.

根据我们的模拟，假定芯片频率为 750 MHz，NSR 训练的实时性能要求每周期从 SRAM 存储器访问 4096 个不规则寻址的 4-byte word 的吞吐量。

为满足这一存储器吞吐量要求，编码表必须至少存储在 4096 个 SRAM 组中。然而，设计连接这些 SRAM 组与后续处理器的 NoC（Network-on-Chip）是一个具有挑战性的问题。NoC 的设计目标是在 SRAM 库和输出寄存器之间正确传输数据，同时保证高吞吐量和低芯片面积成本。

### 4.4.2 Challenges.

![[02-论文笔记/NeuralRendering（神经渲染）/竞品：Cambricon-R/images/Untitled 7.png]]

这个问题具有挑战性，原因有二。首先，该 NoC 所需的高分辨率连接在**芯片面积上非常昂贵**。由于对不同编码表的内存访问是相互独立的，我们可以将存储体和输出寄存器分成 16 组，分别对应 16 个编码表。由于每组都独立工作，我们将在本文后面的内容中把问题从 4096 个 Bank 缩减到 256 个 Bank。每个组内都需要一个大型交叉条（256-to- 256 Fully Connected NoC），以便将数据从 Bank 正确移动到正确的输出位置。其次，严重的 Bank Conflict 会拖累吞吐量。由于每个顶点条目的索引是通过哈希函数计算的，因此每个内存访问请求会随机分配到 256 个库中。因此，一些 Bank 需要处理多个内存访问请求，而另一些 Bank 则不处理任何请求。由于所有 Bank 都需要等待最繁忙的 Bank 完成内存访问，因此吞吐量受到拖累。

当前架构的 NoC 无法很好地解决这些挑战。我们选择三种典型基线来分析它们的性能，并在下文中提出我们的设计方案。

**NN 加速器**

传统 NN 加速器的片上存储器组织和 NoC 不适合执行编码阶段。主要原因是当前 NN 核加速器的片上 I/O 接口是为高吞吐量访问大型矩阵和矢量而设计的。**它们无法为细粒度 I/O 接口提供高吞吐量。**要访问一个 4-byte 的 word，需要从 SRAM 中访问更长的数据宽度（对于典型的 NN 核加速器，需要 32-128 byte），然后丢弃大部分数据。因此，SRAM 的大部分带宽都被浪费了。

**GPU**

尽管 GPU 中的共享内存专为高吞吐量细粒度（4 byte）内存访问而设计（见表 1），但 GPU 也无法高效执行编码阶段。图 8 显示了 GPU 中共享内存的 NoC。在每个 SM 中，32 个存储体提供每个周期 32 个 4 字节字的最大吞吐量。对这 32 个存储体的内存访问通过两个交叉条（一个 32x32 Address Crossbar 和一个 32x32 Data Crossbar）完成。然而，在编码阶段的处理中，每个表必须存储在 8 个 SM 中，才能满足每个周期 256 个字的吞吐量要求。这将导致 SM 之间的大量数据移动，因为共享内存只有同一 SM 中的本地处理器才能看到。而一个 SM 从另一个 SM 的共享内存中访问数据的唯一途径是向全局内存写入和读取。因此，GPU 中的共享内存组织和 NoC 不适合编码阶段的处理。

**Optimized GPU**

这启发我们改进 GPU 的 NoC，看看会发生什么。我们对 GPU 进行了直接改进，将 SRAM 组的组大小从 32 扩大到 256，以避免 SM 之间的数据移动。如图 8(b) 所示，在每个存储组中，改进后的 GPU 基线使用两个 256x256 的 crossbar 进行组内数据移动：一个 Address crossbar 和一个 Data crossbar。 Address crossbar 和 Data crossbar 的连接宽度分别为 11 位 和 32 位。因此，每个 Bank Group 可有效处理 256 次独立的 4-byte word 内存访问，满足了我们对内存访问粒度和访问编码表峰值吞吐量的要求。

然而，这种解决方案有两个缺点。首先，highradix crossbar 的面积成本很高。在我们的实验中，由于存在大量的交叉线，一个银行组的 NoC 在运行 3 周后放置和路由失败。我们通过线性放大 [41] 中报告的结果，估算了图 8[b]中两个 256x256 交叉条的核心面积。在 90 纳米技术下，地址交叉条和数据交叉条分别占据至少 $22\text{mm}^2$ 和 $64\text{mm}^2$ 的核心面积。根据文献，在扩展到 45 纳米技术节点后，两条 crossbar 共占用 $30.68\text{mm}^2$ 的空间。然而，我们使用台积电 45nm 技术对一组 SRAM 组进行合成、放置和布线，得到的总面积成本为 $60.16\text{mm}^2$ 。总之，改进后 GPU 中 NoC 的面积占 SRAM 组面积的 50.9%。

其次，改进后的 GPU 基线也没有解决 Bank Conflict 问题。SM 中的 256 个 Bank 只能同时处理一条指令。因此，内存请求会不规则地分布在所有 Bank 中，导致最繁忙的 Bank 出现 bottleneck，限制了性能。我们进行了一项实验，利用 NSR 训练中的真实内存访问请求来评估改进后 GPU 的吞吐量。结果表明，改进后的 GPU 在一组 256 个存储体中实现的平均吞吐量仅为每个周期 54 个 word ，这意味着仅利用了所有 SRAM 存储体峰值吞吐量的 21.1%。

### 4.4.3 Our Design

为了减轻高分辨率连接昂贵的面积成本和组冲突问题，我们提出了一种名为 Auto-Interpolation Bank Array（AIBA）的架构，用于存储 Encoding Table 和处理 Encoding Computations。图 8(c) 显示了 AIBA 的整体架构。它采用 2Dmesh NoC 拓扑，每个节点连接一个 SRAM 库。因为只有左列节点与Address Crossbar 相连，只有底行节点与 Data Crossbar 相连。这样就避免了耗费大量面积的 High-Radix Crossbar，而且 2D 网状拓扑结构只包含相邻节点之间的短连接，大大减少了连接之间的交叉点。AIBA 的加载/存储指令包含一组由 32 个点产生的 256 个内存请求（每个点的编码需要该点所属网格的 8 个顶点的内存请求）。内存请求通过最左边的一列节点发送到存储体阵列。然后，每个节点将请求传送到右侧的节点。访问的数据被传送到底部的节点。预处理单元用于计算目标节点、顶点地址和插值权重。每个节点的结构如图 8(d) 所示，它包含一个路由横杆、两个用于内存访问请求的请求缓冲区、一个计算单元以及许多其他用于数据路由、顶点插值和结果输出的缓冲区。具体设计和数据流如下

**Asynchronization control mode of banks:**

我们将所有 Bank Group 的控制模式从同步改为异步，从而解决了 Bank Conflict 的问题。对于改进后的 GPU Baseline，一个 Group 中的 256 个 bank 是同步控制的，并且在任何时候都会处理访问 256 个 word 的相同指令。由于入口地址的不规则性，256 个 word 在 256 个 bank 中的分布并不均匀。因此，每个存储体处理的内存访问请求数量不同。因此，该存储组的吞吐量受到最繁忙存储组的限制。

在异步模式下，所有存储体都是独立控制的，新的内存访问指令会被解码并发送到所有存储体，而不再等待旧指令的结束。每个存储体都有一个请求缓冲区，用于缓存冲突的请求，当没有请求到达该存储体时，这些请求可以得到处理。这样就平衡了每个组的繁忙周期和空闲周期。在这种情况下，银行组的整体吞吐量也会受到请求缓冲区大小的限制。当任何一个库的请求缓冲区已满时，下一条指令的解码必须停止，直到有足够的空间来缓存新的请求。我们进行了一次模拟实验，以找出每个组所需的足够缓冲区大小。仿真假定所有库的缓冲区大小都是无限的，因此指令解码不会出现停滞。仿真结果表明，每个节点缓冲区中存储的请求数最多不超过 107 个。在 45 纳米技术节点上，所有这些缓冲器占用的总面积为 $1.69 \text{mm}^2$，与 SRAM 存储体相比，其成本可以忽略不计。

**Interpolation inside the AIBA:**

异步控制模式会导致 Bank Group 输出分配问题。在同步模式下，在一个周期内，Bank Group 输出的条目不超过 256 个，且都属于同一条指令，对应 256 个内存访问请求。但在异步模式下，一个周期内输出的条目可能属于不同的指令（根据我们的模拟，最多可达 77 条指令）。因此，每个输出需要分配到 197127 个可能位置中的一个。要将 256 个输出分配到正确的位置，需要一个 256 × 19712 的 Crossbar switch，这在实现上是不切实际的。

为了解决这一难题，我们采用的方法是在 AIBA 内部完成每 8 个顶点的三线插值，然后输出点的结果条目。这样可以将输出条目的数量减少 8 倍。要实现这一想法，需要将每个顶点的插值权重和 Point ID 打包，并与内存请求一起传输。我们需要设计一个数据流来决定在哪里计算插值和数据移动路径。

![[02-论文笔记/NeuralRendering（神经渲染）/竞品：Cambricon-R/images/Untitled 8.png]]

**AIBA data flow vs. Naive dataflow:**

图 9(a) 显示了数据流的直接设计。数据首先向下移动。数据到达最下面一排节点后，被移动到右侧节点。每个节点都有三个数据源。第一，来自本地 SRAM 的数据。第二，来自顶部节点的数据。第三，来自左侧节点的数据（仅适用于底排节点）。最后两类数据可能包括访问的原始数据、插值的部分结果和插值的最终结果。每个节点首先会通过比较每对不同来源数据的 point_id 来识别属于同一输出点的数据，然后执行三线插值计算，并将数据和结果传送给下一个节点。最后，所有数据和（部分）结果到达右下方节点。

然而，实现这一数据流的成本过于昂贵。实现这一数据流需要在每个节点中安装复杂的匹配电路，以识别哪对数据属于同一个点。匹配电路的面积成本随着需要匹配的矢量长度而增长。图 9(a) 显示了根据我们的模拟，每个节点中需要匹配的向量的最大长度。需要注意的是，存储在右下方节点中的未完成结果数量达到 1688 个，需要一个 248 × 1688 的交叉条，这在实现上是不切实际的。我们提出了 AIBA 数据流，以减轻输出前结果匹配的面积成本。它的设计思路是将图 9(a) 中红色节点的多达 1688 个未完成结果分配到所有 256 个节点。该数据流如图 9（b）所示，其执行过程如下。图 8(c) 展示了使用 AIBA 数据流对一个采样点进行内存访问和插值操作的示例。

1. Pre-processing。预处理单元计算采样点对应的 8 个顶点的地址、插值权重和共享目标节点。目的节点负责累积用于插值的加权编码向量（假设它是图 8(c) 中的节点 (1,1)）。选择目的节点是为了平衡所有节点之间的开销，并最大限度地减少数据移动的总距离。然后，这些数据连同采样点的索引一起打包，通过地址横杆和行连接（图 8(c) 中的红色箭头）发送到节点。）
2. Loading and Transmitting encoding data。收到数据包后，每个节点首先从本地 SRAM 中加载编码数据，并乘以插值权重。然后，节点通过 routing crossbar，沿着图 8(c) 中的黄色箭头，将加权后的编码数据传送到目的地节点。
3. Accumulating partial sum。到达目的节点后，数据包通过目的节点中的交叉条 X1 与缓存的部分和进行匹配。X1 输出匹配的部分和的索引。然后，交叉条 X2 将顶点的加权编码数据传输并添加到匹配的部分结果中。
4. Outputting。如果部分相加已经累积了 8 个顶点，则插值操作完成。结果将通过列连接传送（图 8（c）中的蓝色箭头）。在到达 AIBA 的底层节点后，结果将通过输出横条传送到输出缓冲区的正确地址。最后，MLP 单元可以从所有 AIBA 的输出缓冲区访问编码数据。
5. Backward and Update。在后向处理开始时，采样点的梯度从 MLP 单元传送到预处理单元。预处理单元首先使用图 8(c) 中缓存在点缓冲区中的坐标计算 8 个顶点的地址和插值权重，然后将插值权重乘以编码结果的梯度，得到每个顶点的梯度。之后，8 个顶点的梯度被传送到右侧节点并累积起来。在每个节点的 SRAM 中，前 4KB 用于存储编码数据，后 4KB 用于存储每个编码向量的累积梯度。当节点接收到编码数据的梯度时，节点会将其添加到存储在编码数据地址 + 4KB 的累积梯度中。在每次训练迭代结束时，每个节点都会使用累积梯度更新其 SRAM 中的所有编码矢量。
6. Updating the occupancy grid。在处理占用网格更新时，AIBA 需要执行编码阶段的前向过程，更新后的占用网格将存储在全局缓冲区中，而不是 AIBA 的 SRAM 中。

**Hardward implementation**

图 9(b) 显示了根据我们的模拟，所有节点 X1 和 X2 的 Matching Crossbar 的最大尺寸。我们选择 8x13 作为每个节点的 Matching Crossbar X1 和 X2 的最终尺寸，以便立即处理到达缓冲区中的所有数据。这些匹配横梁的总面积成本为 $1.07\text{mm}^2$（45 纳米）。此外，数据向目的节点移动时，每个节点的路由横梁都会产生大量流量。根据我们的仿真，20x20 的交叉条足以使路由交叉条在不停滞的情况下传输数据。

每个节点完成插值后，将结果垂直传输到底部节点，每个底部节点每个周期输出两个字，每个节点都有一个输出缓冲器，用于缓存拥挤的输出。根据我们的仿真，每个周期从库阵列输出的 32 个字属于不超过 28 条指令。我们使用分层交叉条连接来分配输出数据。第一条 32x32 Crossbar 根据指令将数据分成 32 个缓冲区。然后在每个缓冲区中，使用 32x32 横条将数据分配到正确的位置。这 33 根横条在 45 纳米的制程中总占地面积为 0.91 𝑚𝑚2，与 SRAM 组的占地面积相比是可以接受的。

总之，AIBA 通过异步控制所有 SRAM 存储体，解决了存储体冲突问题。同时，我们通过设计新颖的数据流来优化面积成本，避免因数据拥塞而使用高分辨率连接。它实现了每个周期 29.1 个点的编码吞吐量，即 256 个 SRAM 存储体峰值吞吐量的 91%，而基线仅为 21%。AIBA 的总面积成本为 $71.25\text{mm}^2$ 英寸（45 纳米）。所有 256 个 SRAM 组需要 $60.16\text{mm}^2$ 的面积，而用于缓冲器、小横梁和连接的额外面积总共为 $11.09\text{mm}^2$ 。总之，与所有 SRAM 库的总面积相比，AIBA NoC 只花费了 18.4% 的额外面积，却实现了 91% 的峰值吞吐量，而基线 NoC 花费了所有 SRAM 库 50.9% 的面积，却只实现了 21% 的峰值吞吐量。

# 5. Experimental Evaluation

## **5.1 Methodology**

### 5.1.1 Hardware Implementation

我们使用硬件综合工具和模拟器来测量 Cambricon-R 的性能和能耗。我们用 Verilog 实现了 Cambricon-R 的 RTL 设计。我们使用设计编译器（Design Compiler）和集成电路编译器（IC Compiler）在台积电 45 纳米技术节点上对 Cambricon-R 的关键单元进行综合、布局和布线。由于基准 NVIDIA A100 采用的是 7 纳米技术节点，因此根据文献[31]，我们将面积和功耗结果按比例放大到 7 纳米技术节点。研究论文通常采用这种缩放比较方式。HBM 2.0 的内存访问能耗估计为 3.9 pJ/位[25]。为了进行性能评估，我们用 Python 构建了一个周期精确模拟器，用于测量执行时间、DRAM 访问次数和 PE 利用率。在该模拟器中，通过调用 Ramulator 的 warped 接口来测量内存访问 DRAM 的周期数。我们的模拟器在 8 台配备 AMD EPYC 7742 64 核处理器的服务器上并行运行。我们没有使用 RTL 模拟器来评估性能，因为 NSR 任务有太多操作，而且我们的架构规模太大。为了验证该模拟器的准确性，我们通过运行一个小算法配置和一个小硬件配置，将其与 RTL 模拟器进行比较。验证结果表明，我们模拟器的评估周期比 RTL 模拟器的结果少不超过 6.72%。

### 5.1.2 Experimental Configurations.

我们使用 128 个 MLP 单元实现了 Cambricon-R ，每个单元由两个 32×32 MAC（乘法累加）阵列和一个 384 KB 的本地缓冲区组成。全局缓冲区大小为 2 MB。编码单元有 16 个 AIBA。每个 AIBA 由 256 个节点和 256 个 8KB SRAM 组组成。Cambricon-R 的工作频率为 750 MHz。为了进行公平比较，我们使用带宽为 1555 GB/s 的 HBM2 作为片外内存，这与我们的基准 GPU 相同。表 2 列出了 Cambricon-R 的硬件特性。为了突出 AIBA 的面积效率，我们单独列出了 AIBA 中除 SRAM 之外的其他电路的面积。表 2 中的编码单元（其他）包括预处理单元、计算单元、横杆、缓冲器等的成本。

### 5.1.3 BenchMark

我们评估了来自 6 个场景的 12 个数据集，包括 2 个动态场景和 4 个静态场景。两个动态场景是跳绳和站立。每个动态场景提供 4 个数据集，包含同一人在 4 个不同时间点捕捉到的 4 种不同姿势。同一场景的数据集内容非常相似。为了增加基准的多样性，我们又从 4 个静态场景中选择了 4 个数据集，包括 Fox、Ficus、Hotdog 和 Mic。Fox 数据集来自真实场景，其他三个数据集来自合成场景。此外，我们还使用了典型算法的超参数。密度网络和颜色网络的 MLP 架构分别为（32-64-16）和（32-64-64-3）。共有 16 个编码表，每个表有 256k 条目，特征向量长度为两个 FP16 值。训练批量为 262 144 个有效点，总训练迭代次数设为 250 次。

### 5.1.4 Baseline

我们使用两个基线进行比较，一个是 GPU，另一个是 NN 加速器。我们使用的 GPU 是 NVIDIA A100-PCIE-40GB，其峰值性能为 312 TFLOPs（FP16 密集张量性能），片上总内存为 87.25 MB，片外 HBM2 内存为 40 GB，带宽为 1,555 GB/s。我们使用最先进的软件：tiny-cuda-nn 框架[22]。Tiny-cuda-nn 是针对 NVIDIA GPU 高度优化的 NSR 框架，由 instant-ngp GitHub 项目调用。我们使用的是 GitHub 上截至论文提交截止日期的最新版本代码（提交：89fe4166），没有更改源代码。我们在基线评估中使用了 tiny-cuda-nn 的所有优化。值得注意的是，MLP 阶段有两个版本的 CUDA 内核。我们 尝试了这两个内核，并选择了速度较快的内核（FullyFusedMLP）作为本文的 GPU 基线。

NN 加速器的基线可作为更强的基线，帮助评估增量分解。我们使用类似于 TPU 的架构，并通过周期精确的 python 模拟器加以实现。我们利用基于射线的执行流和完全融合的硬件流水线优化了 NN 加速器基线。否则，巨大的 DRAM 访问量将使比较失去意义。此外，我们还使用了小型收缩阵列，以适应 NSR 中 MLP 的小尺寸。我们为采样、损耗和更新功能添加了专用电路，使这些操作无需返回 CPU。不同阶段的片上缓冲区大小与 Cambricon-R 相同。但是，在 NN 加速器基线中缺少了提前终止和 AIBA。此外，访问 SRAM 的字宽为 32 字节，因为 NN 加速器通常设计用于矢量和矩阵运算，无法为细粒度片上内存访问提供高吞吐量。本文将这一基线称为 NNA-opt。为了进行公平比较，我们在表 3 中列出了 Cambricon-R 和基线的硬件规格。

## **5.2 Evaluation Results**

我们将 Cambricon-R 与基线进行了性能、能耗、DRAM 访问和 PE 利用率方面的比较。之后，我们进行了一项消融研究，以明确提前终止和 AIBA 的有效性。请注意，我们的解决方案没有改变 NSR 算法的计算，因此结果应与基线相同。这些基准的平均 PSNR 为 29.14，图像渲染质量可以接受。

![[02-论文笔记/NeuralRendering（神经渲染）/竞品：Cambricon-R/images/Untitled 9.png]]

### 5.2.1 Performance

图 10 显示了整个训练过程的运行时间，包括在 Cambricon-R 和基线上运行的 12 个基准的 250 次训练迭代。J1-J4 代表 Jumpingjacks 中的四帧，S1-S4 代表 Standup，Fx 代表 Fox，Fc 代表 Ficus，Hd 代表 Hotdog，Mc 代表 Mic。结果表明，Cambricon-R 比 GPU 和经过优化的 NN 加速器平均分别提高了 373.8 倍和 6.73 倍。需要注意的是，Cambricon-R 实现了每秒 69.0 个场景的实时吞吐量。由于基于光线的执行流消除了大部分 DRAM 访问量，经过优化的 NN 加速器也比 GPU 加速了 54.9 倍。需要注意的是，不同场景的性能看起来很相似，因为我们的目标算法是迭代次数和批量大小固定的训练任务。

图 11 显示了 Cambricon-R 在 12 个基准测试中与基准测试相比的 PE 利用率。它表明，Cambricon-R 成功地解决了 GPU 利用率低的问题，将平均 PE 利用率从 0.22% 提高到 67.3%。可以发现，在 CambriconR 上运行时，不同数据集的利用率在 49.7% 到 75.73% 之间变化很大，而在 GPU 上运行时，利用率稳定在 0.22% 0.23%。这是因为不同场景的整体空间密度变化范围较大。在训练的稳定期，所有场景中每条射线的平均有效点都有很大差异。福克斯数据集来自复杂度最高的真实场景，因此光线中的平均有效点数约为 78 个。而其他数据集都来自空间结构简单的合成场景。例如，Mic 数据集平均每条射线中只有 16 个有效点。这只是我们早期终止执行管道执行粒度（32 点）的一半。由于 Cambricon-R 是以逐条光线流的方式执行的，因此对于这些场景结构简单的数据集来说，会造成一定的性能浪费。对于 GPU 来说，它以逐级流程执行，整个批次的点总数是固定的（262,144）。因此，每条射线中的点数不会影响性能。

图 12 显示了 Cambricon-R 与基线在 DRAM 访问方面的比较。图中显示，Cambricon-R 减少了 99.995% 的 DRAM 访问量。Cambricon-R 从 DRAM 或向 DRAM 平均只访问 50.98 MB 的数据。这主要是因为完全融合的 Cambricon-R 将所有中间数据存储在芯片上，消除了 GPU 不同阶段之间多余的 DRAM 中间数据访问，只保留必要的 DRAM 访问，如原始训练数据和训练参数。这也是图 11 所示 Cambricon-R 处理元件利用率极高的原因。

图 13 显示了不同阶段不同单元的运行时间明细。每个归一化运行时间都是相对于一次迭代的总运行时间而言的。由于同一阶段中不同单元的计算采用流水线方式，因此一个阶段的总运行时间与耗时最长的单元的运行时间接近。前向阶段、后向阶段、更新阶段和占用率更新阶段分别占总运行时间的 35.3%、49.2%、4.0% 和 11.5%。在前向和后向阶段，瓶颈都在于 MLP 单元。在更新阶段，瓶颈在于编码表的更新。在前向阶段，虽然实现了提前终止，但如第 4.3 节所述，每条射线可执行的无效点仍不超过 31 个。因此，后向阶段的编码运行时间略短于前向阶段。而 MLP 在后向阶段耗时增加的原因是，与 MLP 前向阶段相比，MLP 后向阶段每个采样点的计算量更大。在占位更新阶段，MLP 单元只计算密度网络。因此，编码单元比 MLP 单元耗时更多。

### 5.2.2 Energy

如图 14 所示，我们报告了 CambriconR 和基线在 12 个基准上处理 250 次迭代训练时的总体能耗。结果显示，与 GPU 相比，Cambricon-R 的能耗平均降低了 256.6 倍。请注意，GPU 的能耗是通过运行时间与 nvidia-smi profiler 工具报告的平均功率相乘计算得出的。优化后的 NN 加速器的平均能耗比 Cambricon-R 高出 5.50 倍。原因有两个方面。首先，由于没有提前终止，它在无效点上浪费了一些操作。其次，在处理细粒度的 4 字节宽内存请求时，NN 加速器片上内存的 32 字节字宽造成了能源浪费。

我们进一步细分了 CambriconR 的能耗，结果如图 15 所示。能耗细分为 5 部分，分别是采样单元、编码单元、MLP 单元和片外内存访问的能耗。为了评估 AIBA 中 NoC 的能效，我们将编码单元的能耗进一步细分为两部分，即库阵列和 NoC。值得注意的是，片外内存访问的能耗平均只占总能耗的 0.08%，这表明 Cambricon-R 去掉了大部分片外内存流量。能耗最大的部分是对编码表的内存访问，因为对编码表的内存访问量巨大。AIBA NoC 的能耗仅占总能耗的 1.5%。这证明 NoC 的能源开销可以忽略不计。

### 5.2.3 Optimization Analysis

为了明确我们设计的有效性，我们对包括推理中的早期终止（ETI）和 AIBA 在内的两种优化进行了消减研究。我们对 Cambricon-R-plain、Cambricon-R-plain（含 ETI）、Cambricon-R-plain（含 AIBA）和 CambriconR 进行了消减研究。图 16 显示了每种解决方案与 Cambricon-R-plain 相比的速度提升。与 Cambricon-R-plain 基线相比，ETI 和 AIBA 的优化组合平均整体提速 4.25 倍。仅 ETI 优化就提高了 1.73 倍，这表明 ETI 有效地节省了无效点推理的计算量。AIBA 优化仅提高性能 3.06 倍，这是通过提高访问编码表的吞吐量实现的。这项消融研究证明了我们对 ETI 和 AIBA 关键优化的有效性。

# 6. Related Works

居然没有引我们的，差评，没眼光

# 7. Conclusion

本文提出的 Cambricon-R 是第一个将 NSR 算法推向实时应用的加速器。Cambricon-R 是一种完全融合的片上处理架构，采用新颖的基于射线的执行模型，消除了 99.995% 以上的片外内存访问。为了提高 Cambricon-R 的效率，我们进一步提出在推理过程中提前终止，以避免在前向过程中浪费对无效点的计算，并将性能提高 2 倍。我们还提出了自动插值库阵列，在很大程度上缓解了库冲突造成的吞吐量浪费，将库阵列的吞吐量利用率从 21% 提高到 91%。实验结果表明，与英伟达 A100 GPU 上的先进解决方案相比，Cambricon-R 实现了 373.8 倍的提速和 256.6 倍的节能。更重要的是，它能以每秒 69.0 个场景的速度实时呈现 3D 场景。