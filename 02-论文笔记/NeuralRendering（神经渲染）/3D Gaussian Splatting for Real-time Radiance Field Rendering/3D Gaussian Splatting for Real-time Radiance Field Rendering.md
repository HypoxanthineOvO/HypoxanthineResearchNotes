# 0. Abstract
Radiance Field 方法最近在多张照片或视频捕捉的场景的新视角合成方面取得了突破性进展。然而，要实现高视觉质量仍然需要耗费大量训练和渲染成本的神经网络，而近期的快速方法则不可避免地在速度上对质量进行了折衷。对于无边界和完整的场景（而不仅仅是孤立的物体）以及1080p分辨率的渲染，目前没有任何方法能够实现实时显示。我们引入了三个关键要素，使我们能够在保持竞争力的训练时间的同时实现最先进的视觉质量，并且重要的是允许以 1080P 分辨率进行高质量的实时（ $≥ 30 \text{ FPS}$）新视角合成。首先，从相机校准时生成的稀疏点开始，我们用3D高斯函数来表示场景，以在场景优化中保留连续体辐射场的良好特性，同时避免在空白空间进行不必要的计算；其次，我们执行交替优化/密度控制的3D高斯函数，特别是优化各向异性协方差以实现对场景的准确表示；第三，我们开发了一种快速的可见性感知渲染算法，支持各向异性的斑点投射，并加速训练并实现实时渲染。我们在几个已建立的数据集上展示了最先进的视觉质量和实时渲染效果。
# 1. Introduction
Meshes 和 Points 是最常见的3D场景表示方法，因为它们是显式的的，并且非常适合 Fast GPU/CUDA-Based 的光栅化技术。相反，最近的神经辐射场（NeRF）方法建立在连续场景表示的基础上，通常使用 Volumetric ray-marching 的方式来优化多层感知器（MLP），以实现对捕捉场景的新视角合成。同样，迄今为止最有效的辐射场解决方案是基于连续表示的，通过插值存储在体素（PlenOctrees）或哈希（Instant-NGP）网格或点（Point-NeRF）中的值。尽管这些方法的连续性有助于优化，但渲染所需的随机采样代价高昂，可能会产生噪声。我们引入了一种新方法，将两者的优点结合起来：我们的3D高斯表示允许在最先进的视觉质量和竞争力训练时间下进行优化，而我们 tile-based 的斑点投射解决方案确保了在几个先前发布的数据集上以最先进的质量实时渲染，分辨率为1080P（见 [[3D Gaussian Splatting for Real-time Radiance Field Rendering]] ）。
![[02-论文笔记/NeuralRendering（神经渲染）/3D Gaussian Splatting for Real-time Radiance Field Rendering/images/Untitled.png|Untitled.png]]
图1. 我们的方法在只需要与最快的先前方法相竞争的优化时间的情况下，实现了与前一方法相同质量的辐射场实时渲染。这种性能的关键在于一种新颖的3D高斯场景表示结合实时可微分渲染器，它对场景优化和新视角合成都提供了显著的加速。需要注意的是，与InstantNGP相比，在相当的训练时间内，我们实现了与他们类似的质量；虽然这是他们达到的最高质量，但通过51分钟的训练，我们实现了最先进的质量，甚至稍微好于Mip-NeRF 360。
我们的目标是允许对使用多张照片捕捉的场景进行实时渲染，并且以与先前最高效的方法相当的优化时间创建这些表示。最近的方法实现了快速训练，但难以达到当前最先进的NeRF方法（例如 Mip-NeRF 360）所获得的视觉质量，该方法需要长达48小时的训练时间。快速但质量较低的辐射场方法可以在场景依赖下实现交互式渲染时间（每秒10-15帧），但无法实现高分辨率的实时渲染。
总结起来，我们提供了以下贡献：
- 引入各向异性的三维高斯函数作为高质量、非结构化的辐射场表示。
- 一种优化三维高斯函数属性的方法，与自适应密度控制交错进行，为捕捉到的场景创建高质量的表示。
- 一种对GPU进行快速且可微分的渲染方法，具有可见性感知、允许各向异性喷溅和快速反向传播的特点，以实现高质量的新视角合成。
通过对先前发布的数据集进行实验，我们的结果表明，我们可以从多视图捕获优化我们的三维高斯函数，并且在质量上达到或超过最好的隐式辐射场方法。我们还可以实现与最快方法相似的训练速度和质量，并且重要的是，为新视角合成提供了首个具有高质量的实时渲染。
# 2. Related Works
首先，我们简要介绍传统重建方法，然后讨论 Point-Based Rendering 和 Radiance Fields 方法，并讨论它们之间的相似之处；辐射场是一个广阔的领域，因此我们只关注直接相关的研究工作。要全了解该领域，请参阅最近出色的综述文章。
## 2.1. Traditional Scene Reconstruction and Rendering
最初的 Novel-View Synthesis 方法基于 Light Fields ，首先进行 Densely Sampled ，然后允许Unstructured Capture。Structure-from-Motion（SfM）的出现开创了一个全新的领域，即可以使用照片集来合成新颖的视图。SfM 在相机校准过程中估计了一个稀疏的点云，最初用于简单的三维空间可视化。随后的多视角立体（MVS）多年来产生了令人印象深刻的全三维重建算法，使得多种视图合成算法得以发展。所有这些方法都将输入图像 Re-Project 并 Blending 到新视角相机中，并使用几何图形来指导这种重新投影。这些方法在许多情况下都取得了很好的效果，但通常无法完全恢复未重建区域或 "over-reconstructed"（当MVS生成不存在的几何体时）。最近的神经渲染算法大大减少了这种伪影，并避免了在GPU上存储所有输入图像的巨大成本，在大多数方面都优于这些方法。
## 2.2. Neural Rendering and Radiance Fields
早期采用了深度学习技术进行 Novel-View Synthesis；卷积神经网络（CNN）被用于估计混合权重或纹理空间解决方案。其中大多数方法的主要缺点是采用了基于多视角立体匹配的几何结构；此外，使用CNN进行最终渲染经常会导致时间上的闪烁问题。
用于 Novel-View Synthesis 的体积表示由 Soft3D 公司提出；随后，在连续可变密度场表示几何图形的基础上，提出了与体积光线行进相结合的深度学习技术。由于查询体积需要大量样本，因此使用体积光线行进进行渲染的成本很高。神经辐射场（NeRFs）引入了重要性采样和位置编码以提高质量，但使用了大型多层感知器，对速度产生了负面影响。NeRF的成功催生了大量解决质量和速度问题的后续方法，这些方法通常通过引入正则化策略来实现；Mip-NeRF360是目前 Novel-View Synthesis 图像质量方面最先进的方法。虽然渲染质量非常出色，但训练和渲染时间仍然非常长；我们能够在提供快速训练和实时渲染的同时，达到甚至超过这一质量。
最近的方法主要通过利用以下三种设计选择来加快训练和/或渲染速度：使用空间数据结构来存储（神经）特征，这些特征随后在体积射线行进过程中进行内插；不同的编码；以及MLP容量。 这些方法包括不同的空间离散化变体、编码本和编码方式（如哈希表），允许使用较小的MLP或完全放弃神经网络。
这些方法中最值得注意的是InstantNGP，它使用散列网格和占位网格来加速计算，并使用较小的MLP来表示密度和外观；Plenoxels使用稀疏体素网格来插值连续密度场，并能够完全放弃神经网络。这两种方法都依赖于球谐波：前者直接表示方向效应，后者将其输入编码到颜色网络。虽然这两种方法都能提供出色的结果，但这些方法仍难以有效地表现空域，部分取决于场景/捕捉类型。此外，图像质量在很大程度上受限于用于加速的结构化网格的选择，而渲染速度则受限于需要为给定的光线行进步骤查询许多样本。我们使用的非结构化、显式GPU友好的三维高斯在没有神经组件的情况下实现了更快的渲染速度和更好的质量。
## 2.3. Point-Based Rendering and Radiance Fields
基于点的方法可有效渲染断开的 unstructured geometry samples（即点云）。在最简单的形式中，点采样渲染光栅化了一组具有固定大小的 unstructured points，为此它可以利用图形 API 原生支持的点类型或 GPU 上的并行软件栅格化。点采样渲染虽然忠实于底层数据，但却存在漏洞，会造成 aliasing ，而且严格来说是不连续的。基于点的高质量渲染的开创性工作通过“拼接”范围大于像素的 Point Primitives（例如圆形或椭圆形圆盘、椭圆体或曲面）来解决这些问题。
最近，人们对 **Differentiable** Point-based Rendering 技术产生了兴趣。使用 Neural Features 增强点并使用 CNN 渲染，可实现快速甚至实时的视图合成；然而，它们仍然依赖于 MVS 来获得初始几何图形，因此继承了MVS的缺陷，最明显的是在无特征/有光泽区域或薄结构等困难情况下的过度或不足重构。
Point-based $\alpha$-blending 和 NeRF-style 的 Volumetric Rendering 在本质上共享相同的 Image formation model。具体来说，颜色 $C$ 是通过沿光线的 Volumetric Rendering 给出的：
$$C = \sum_{i=1}^NT_i(1-\exp(-\sigma_i\delta_i))\mathbf c_i\quad\text{with}\quad T_i=\exp\left(-\sum_{j=1}^{i-1}\sigma_j\delta_j\right)$$
其中，采样点的密度 $\sigma$，透射率（Transmittance） $T$ 和颜色 $\mathbf c$ 是沿着光线按间隔 $\delta_i$ 采样得到的。这可以被改写成：  
  
$$C=\sum_{i=1}^NT_i\alpha_i\mathbf{c}_i$$
这里有：
$$\alpha_i=(1-\exp(-\sigma_i\delta_i))\quad\text{and}\quad T_i=\prod_{j=1}^{i-1}(1-\alpha_i)$$
一种典型的基于神经点的方法（例如，Neural Point Catacaustics for Novel-View Synthesis of Reflections.）通过混合重叠像素的 $N$ 个有序点来计算像素的颜色 $𝐶$：
$$C=\sum_{i\in\mathcal{N}}c_i\alpha_i\prod_{j=1}^{i-1}(1-\alpha_j)$$
其中 $\mathbf c_i$ 是每个点的颜色，而 $\alpha_i$ 是由计算一个协方差为 $\Sigma$ 的二维高斯分布乘上学习的点的不透明度得出的。
从公式2和公式3中，我们可以清楚地看到图像形成模型是相同的。然而，渲染算法却截然不同。NeRFs是一种隐含地表示空/占空间的连续表示；需要昂贵的随机取样来找到式2中的样本，因此会产生噪声和计算费用。与此相反，点是一种 unstructued 的 discrete representation，具有足够的灵活性，允许创建、删除和移动与 NeRF 类似的几何体。这可以通过优化不透明度和位置来实现，如之前的工作所示，同时避免了全体积表示法的缺点。
Pulsar 实现了快速的 sphere-rasterization ，这启发了我们 tile-based and sorting 的渲染器。然而，根据上述分析，我们希望在排序的 splats 上保持（近似）传统的 $\alpha$-blending，以获得体积表示的优势： 我们的 Rasterization 尊重可见性顺序，这与他们不依赖顺序的方法截然不同。此外，我们在像素中的所有 splats 上反向传播梯度，并对各向异性的 splats 进行 rasterization。这些因素都有助于提高我们结果的视觉质量（见第 7.3 节）。此外，上述方法也使用 CNN 进行渲染，这导致了时间上的不稳定性。尽管如此， Pulsar 和 ADOP 的渲染速度仍是我们开发快速渲染解决方案的动力。
Neural Point Catacaustics 的 Diffuse Point-based Rendering track 侧重于 Specular Effects ，通过使用 MLP 克服了这种时间上的不稳定性，但仍然需要 MVS 几何体作为输入。该类别中最新的方法不需要 MVS ，也使用 SH 作为方向；但是，它只能处理一个物体的场景，并且需要 Masks 作为初始化。虽然该方法在小分辨率和低点计数时速度较快，但目前还不清楚它如何能够扩展到典型数据集的场景中。我们使用三维高斯来实现更灵活的场景表示，避免了对MVS几何的需求，并通过我们的 tile-based rendering algorithm for the projected Gaussians 实现了实时渲染。
最近的一种方法使用 Point 表示辐射场，采用 radial basis approach 。它们在优化过程中采用了点剪枝和致密化技术，但使用的是体积光线行进技术，无法达到实时渲染。
在人体表现捕捉领域，3D Gaussian 被用于表示捕捉到的人体；最近，它们被用于视觉任务中的 Volumetric ray-marching。Neural volumetric primitives 也在类似的背景下被提出。虽然这些方法启发了我们选择 3D Gaussian 作为场景表示，但它们侧重于重建和渲染单个孤立物体（人体或面部）的特定情况，导致场景的深度复杂度较小。相比之下，我们对各向异性协方差的优化、交错优化/密度控制以及高效的深度排序渲染，使我们能够处理包括背景在内的完整复杂场景，无论是在室内还是室外，并且具有较大的深度复杂性。
# 3. OverView
![[02-论文笔记/NeuralRendering（神经渲染）/3D Gaussian Splatting for Real-time Radiance Field Rendering/images/Untitled 1.png|Untitled 1.png]]
图2. 优化从稀疏的SfM点云开始，创建一组3D高斯函数。然后我们对这组高斯函数进行优化和自适应控制密度。在优化过程中，我们使用快速的 tile-based 的渲染器，相比于最先进的快速辐射场方法，能够获得竞争性的训练时间。一旦训练完成，我们的渲染器允许对各种场景进行实时导航。

> 体积阵列（Volumetric splats）是一种用于表示物体体积的渲染技术。它将物体体积划分为一系列小的体素，每个体素对应一个具有位置、大小、颜色和透明度等属性的图像平面（splats）。这些图像平面可以在渲染过程中进行混合和叠加，从而生成最终的图像。体积阵列常用于实时渲染和体积可视化领域，能够有效地表示复杂的物体形状和光照效果。它具有较低的内存消耗和快速的渲染速度，因此在计算资源较有限的系统中得到广泛应用。

> Anisotropic splats（各向异性阵列）是一种扩展了传统体积阵列渲染技术的方法。与传统的体积阵列中的图像平面（splats）一样，各向异性阵列也使用小的平面来表示物体的体积。但是，各向异性阵列允许每个平面的大小和形状根据局部几何特征而异，而不是固定的正方形或矩形。
> 
> 通过使用不同大小和形状的平面，各向异性阵列能够更精确地表示物体的几何细节和方向性特征。例如，在渲染具有细长形状的物体时，各向异性阵列可以使用长而窄的平面来更好地捕捉其形状。这可以提高渲染的质量，并减少细长物体显示为扁平的效果。
> 
> 总的来说，各向异性阵列是一种改进的体积阵列渲染技术，通过适应性地调整平面的大小和形状，实现更准确和真实的渲染结果。它在处理具有方向性特征的物体时特别有用，在计算机图形学和可视化领域得到广泛应用。
我们方法的输入是一组静态场景的图像，以及由 SfM 校准的相应相机，该过程会产生一个稀疏点云。从这些点云中，我们创建了一组三维高斯函数（第4节），由位置（均值）、协方差矩阵和不透明度 $\alpha$ 定义，从而实现了非常灵活的优化策略。这样可以相对紧凑地表示三维场景，部分原因是高度各向异性（anisotropic）的 Volumetric Splats 可用于紧凑地表示细微结构。辐射场的 Directional Appearance Component（Color）通过球谐函数（SH）表示，遵循 Standard Practice。然后，我们的算法通过一系列优化步骤（第5节）来创建辐射场表示，即对3D高斯函数参数（位置、协方差、 $\alpha$ 和 SH Coefficients）进行交叉操作，并进行自适应控制高斯函数密度。我们方法的效率关键在于 time-based 的 Rasterizer（第6节），它允许使用各向异性阵列的 $\alpha$ blending，通过快速排序保持可见性顺序。我们的 Fast Rasterizer 还通过跟踪累积的 $\alpha$ 值来进行快速反向传播，而没有对可以接收梯度的高斯函数数量进行限制。我们方法的概述如 [[3D Gaussian Splatting for Real-time Radiance Field Rendering]] 所示。
# 4. Differentiable 3D Gaussian Splatting
我们的目标是优化一种场景表示，能够从稀疏的（SfM）点集（不包含法线）开始，实现高质量的新视角合成。为了做到这一点，我们需要一种 Primitive，既继承了可微分的体积表示的特性，同时又是 unstructured 和 explicit，以实现非常快速的渲染。我们选择了三维高斯函数，它们是可微分的，并且可以轻松投影到二维的平面上，以便进行快速的渲染中的 $\alpha$-blending。
我们的表示方法与之前使用2D点的方法相似，假设每个点是一个具有法线的小平面圆。鉴于 SfM Points 的极端稀疏性，估计法线是非常困难的。同样，从这样的估计中优化非常嘈杂的法线将非常具有挑战性。因此，我们将几何形状建模为一组不需要法线的三维高斯函数。我们的高斯函数由一个在世界空间中定义的完整的 3D 协方差矩阵 $Σ$ 定义，以点（均值） $\mu$ 为中心：
$$G(x)=\exp\left(-\frac{1}{2}x^T\Sigma^{-1}x\right)$$
这个高斯分布在 Blending 过程中乘以 $\alpha$。
然而，我们需要将我们的三维高斯函数投影到二维空间进行渲染。Zwicker等人演示了如何将其投影到图像空间中。给定一个视图变换 $𝑊$，协方差矩阵 $Σ'$ 在相机坐标系中的表示如下：
$$\Sigma' = JW\Sigma W^TJ^T$$
其中 $𝐽$ 是投影变换的 Affine Apprxomation 的 Jacobian Matrix。Zwicker等人还表明，如果我们跳过 $Σ'$ 的第三行和第三列，我们将得到一个 $2×2$ 的方差矩阵，具有与以前工作中的具有法线的平面点相同的结构和属性。
一个明显的方法是直接优化协方差矩阵 $Σ$ ，以获得表示辐射场的三维高斯分布。然而，协方差矩阵只有在正半定时才有物理意义。对于我们参数的优化，我们使用了梯度下降，很难将其限制在产生有效矩阵的范围内，更新步骤和梯度很容易产生无效的协方差矩阵。
因此，我们选择了一种更直观、同样富有表达力的表示方式进行优化。3D高斯分布的协方差矩阵 $Σ$ 类似于描述椭球体的配置。给定缩放矩阵 $𝑆$ 和旋转矩阵 $𝑅$ ，我们可以找到相应的 $Σ$ :
$$\Sigma = RSS^TR^T$$
为了允许独立优化这两个因素，我们将它们分别存储：一个三维向量 $𝑠$ 用于缩放，一个四元数 $q$ 用于表示旋转。这些可以简单地转换为它们各自的矩阵，并进行组合，确保归一化 $𝑞$ 以获得有效的单位四元数。
为了避免由于训练期间的自动微分而导致的显著开销，我们明确地计算所有参数的梯度。关于精确导数计算的详细细节请参考附录A。
这种适用于优化的各向异性协方差表示方法使我们能够优化三维高斯函数，使其适应捕捉场景中不同形状的几何形状，从而得到一个相当紧凑的表示。图3说明了这种情况。
![[02-论文笔记/NeuralRendering（神经渲染）/3D Gaussian Splatting for Real-time Radiance Field Rendering/images/Untitled 2.png|Untitled 2.png]]
图3. 我们通过将3D高斯函数缩小60%（最右边）来可视化优化后的结果。这清晰地展示了优化后紧凑表示复杂几何形状的各向异性3D高斯函数的形状。最左边是实际渲染的图像
# 5. Optimization With Adaptive Density Control Of 3D Gaussians
我们方法的核心是优化步骤，它通过创建密集的3D高斯函数集合来准确表示场景，以用于自由视点合成。除了位置 $𝑝$ 、 $\alpha$ 和协方差 $Σ$ 之外，我们还优化代表每个高斯函数颜色 $𝑐$ 的球谐系数SH，以正确捕捉场景的视角相关外观。这些参数的优化与控制高斯函数密度的步骤交替进行，以更好地表示场景。
## 5.1. Optimization
优化过程基于对渲染结果图像与捕获数据集中的训练视图进行比较的连续迭代。由于 3D 到 2D 投影的不确定性，几何体的位置可能会出现错误。因此，我们的优化需要能够创建几何体，同时也能够销毁或移动错误位置的几何体。三维高斯函数的协方差参数的质量对表示的紧凑性至关重要，因为大面积均匀区域可以用少量的大型各向异性高斯函数来捕捉。
我们使用随机梯度下降技术进行优化，充分利用了标准的GPU加速框架，并且可以为某些操作添加自定义的 CUDA 核心，遵循最新的最佳实践。特别是我们快速的光栅化（参见第6节）对于优化的效率至关重要，因为它是优化的主要计算瓶颈。
我们对 $\alpha$ 使用 Sigmoid 激活函数，将其限制在 $[0-1)$ 范围内以获得平滑的梯度，对于协方差的 scale ，我们使用指数激活函数。
我们将初始协方差矩阵估计为一个各向同性高斯函数，其轴等于与最近三个点的距离的均值。我们使用类似于 Plenoxels 的标准指数衰减调度技术，但仅用于位置。损失函数采用 L1 和 D-SSIM 项的组合：
$$\mathcal{L} = (1-\lambda)\mathcal{L} + \lambda\mathcal{L}_{\text{D-SSIM}}$$
我们在所有测试中使用 $\lambda = 0.2$ 。我们在第 7.1 节中提供了学习计划和其他细节的详细信息。
## 5.2. Adaptive Control of Gaussians
我们从 SfM 的初始稀疏点集开始，然后运用我们的方法，自适应地控制高斯函数的数量和其单位体积上的密度，使我们能够从初始的稀疏高斯函数集合过渡到更密集的集合，更好地表示场景，并具有正确的参数。在优化预热阶段之后（参见第7.1节），我们每100次迭代密集化一次，并移除透明度几乎为零的高斯函数，即 $\alpha$ 小于阈值 $\epsilon_\alpha$ 的高斯函数。
我们对高斯函数的自适应控制需要填充空区域。它专注于缺失几何特征的区域（"under-reconstruction"），但也会处理高斯函数覆盖场景中大面积区域的情况（通常对应于"over-reconstruction"）。我们观察到这两种情况都具有较大的视图空间位置梯度。直观地说，这可能是因为它们对应于尚未很好重构的区域，优化试图将高斯函数移动来修正这一点。
由于这两种情况都是进行密集化的良好候选，我们将视图空间位置梯度平均幅值大于阈值 $\tau_{\text{pos}}$ 的高斯函数进行密集化，我们在测试中将其设为 $0.0002$ 。
接下来，我们详细介绍这个过程，如 [[3D Gaussian Splatting for Real-time Radiance Field Rendering]] 所示。
![[02-论文笔记/NeuralRendering（神经渲染）/3D Gaussian Splatting for Real-time Radiance Field Rendering/images/Untitled 3.png]]
图4. 我们的自适应高斯密集化方案。顶部行（正在重建）：当小尺度几何（黑色轮廓）覆盖不足时，我们复制相应的高斯函数。底部行（过度重建）：如果小尺度几何由一个大的斑点表示，我们将其分割为两个。
对于处于尚未很好重构区域的小高斯函数，我们需要覆盖必须创建的新几何体。为此，最好是克隆高斯函数，只需创建一个相同大小的副本，并将其沿位置梯度方向移动。
另一方面，高方差区域中的大高斯函数需要分割成较小的高斯函数。我们用两个新的高斯函数替换这样的高斯函数，并将其尺度除以经验确定的 $\phi = 1.6$ 倍。我们还通过使用原始的3D高斯函数作为概率密度函数进行采样来初始化其位置。
在第一种情况下，我们检测并处理增加系统总体积和高斯函数数量的需求，而在第二种情况下，我们保持总体积不变，但增加高斯函数的数量。与其他体积表示方法类似，我们的优化可能会在靠近输入摄像机的浮点误差中陷入困境；在我们的情况下，这可能会导致高斯函数密度不合理增加。调节增加高斯函数数量的有效方法是在每个 $𝑁 = 3000$ 次迭代中将 $\alpha$ 值设为接近于零。然后，优化程序会在需要的高斯函数增加 $\alpha$ 值的同时，允许我们的剔除方法删除 $\alpha$ 小于 $\epsilon_\alpha$ 的高斯函数，如上所述。高斯函数可能会收缩或增长，并与其他函数有较大重叠，但我们定期删除在世界空间中非常大或在视图空间中占据较大面积的高斯函数。这种策略能够很好地控制总高斯函数数量。我们模型中的高斯函数始终保持在欧几里得空间中的原始形式；与其他方法不同，我们不需要对远距离或大范围高斯函数进行空间压缩、变形或投影策略。
# 6. Fast Differentiable Rasterizer for Gaussians
我们的目标是实现快速的整体渲染和快速排序，以便进行近似的 $\alpha$-blending，包括各向异性阵列，并避免前期工作中存在的限制 Splatting 数量的硬性限制。
为了实现这些目标，我们设计了一个 tile-based 的光栅化器，受到最近的软件光栅化方法的启发，以便一次对整个图像进行原始排序，避免了每个像素排序的开销，该开销阻碍了先前的 $\alpha$-blending 解决方案。我们快速的光栅化器可以在低额外内存消耗下对任意数量的混合高斯函数进行高效的反向传播，每个像素只需要恒定的额外开销。我们的光栅化管道是完全可微的，并且在给出2D投影（第4节）的情况下，可以类似于先前的2D斑点方法对各向异性阵列进行光栅化。

> [!info] Tile-Based Rendering学习笔记  
> 现代移动端图形体系结构的概述现代SoC通常会同时集成CPU和GPU。 CPU被用于处理需要低内存延迟的序列、大量分支的数据集，其晶体管用于流控制和数据缓存。 GPU为处理大型，未分支的数据集，如3D渲染。晶体管专用于…  
> [https://zhuanlan.zhihu.com/p/393712805](https://zhuanlan.zhihu.com/p/393712805)  
我们的方法首先将屏幕分成 16×16 个 tile，然后针对 view-frustum 和每个 tile 对3D高斯函数进行裁剪。具体而言，我们只保留与 view-frustum 相交的置信区间为99%的高斯函数。此外，我们使用一个警戒区来轻松拒绝在极端位置上的高斯函数（即，那些均值接近 near plane 和远离 view-frustrum 的高斯函数），因为计算它们的二维协方差的投影将不稳定。然后，我们根据它们重叠的 tile 数量实例化每个高斯函数，并为每个实例分配一个结合了 view-space depth 和 tile-ID的关键字。然后，我们使用单个快速的 GPU Radix Sort 根据这些关键字对高斯函数进行排序。请注意，没有额外的像素排序， blending 是基于这个初始排序进行的。因此，在某些配置下，我们的 $\alpha$-blending 可能是近似的。然而，当阵列接近单个像素的大小时，这些近似变得可以忽略不计。我们发现，这个选择极大地提高了训练和渲染性能，并没有在收敛场景中产生可见的伪影。

> [!info] GPU排序（基数排序Radix Sort）小节【理论篇】  
> 0.  
> [https://zhuanlan.zhihu.com/p/491798194](https://zhuanlan.zhihu.com/p/491798194)  
在对高斯函数进行排序后，我们通过识别对应 tile 发射的首个和最后一个按深度排序的条目，为每个 tile 生成一个列表。对于光栅化，我们为每个 tile 启动一个 thread block 。每个 block 首先协作地将 Packets of Gaussians 加载到共享内存中，然后对于给定的像素，通过从前到后遍历list 来 integrate 颜色和 $\alpha$ 值，从而最大限度地增加数据加载/共享和处理的并行性。当我们在像素中达到目标饱和度时，相应的线程停止。定期查询 tile 中的线程，并且在所有像素饱和（即 $\alpha$ 变为1）时终止整个 tile 的处理。附录C中给出了排序的详细信息和整体光栅化方法的高级概述。
在光栅化过程中， $\alpha$ 的饱和是唯一的停止准则。与先前的工作相比，我们不限制接收梯度更新的混合图元的数量。我们强制执行这个属性，以便让我们的方法处理具有任意变化深度复杂度的场景并准确地学习它们，而不必诉诸于场景特定的超参数调优。在反向传播过程中，我们因此需要恢复正向传播中每个像素的混合点的完整序列。一种解决方案是在全局内存中存储每像素的任意长的混合点列表。为了避免暗示的动态内存管理开销，我们选择再次遍历每个 tile 的列表；我们可以重复使用正向传递的高斯函数和 tile 范围的排序数组。为了方便梯度计算，我们现在从后向前遍历它们。
遍历始于影响 tile 中任意像素的最后一点，再次协作地将点加载到共享内存中。此外，只有在像素的深度低于或等于正向传递过程中贡献其颜色的最后一点的深度时，每个像素才会开始（昂贵的）重叠测试和点的处理。在第4节中描述的梯度计算需要在原始混合过程的每个步骤中累积的不透明度值。与在反向传播中遍历逐渐减小的不透明度的显式列表相比，我们可以通过仅在正向传播结束时存储总累积不透明度来恢复这些中间不透明度。具体而言，每个点在正向过程中存储最终累积的不透明度 $\alpha$ ；我们将此除以我们的从后向前遍历中每个点的 $\alpha$ ，以获得梯度计算所需的系数。
---
  
# 【后续为具体实现和实验结果】