# 1. Abstract
尽管使用更快、更深的卷积神经网络在单图像超分辨率的准确性和速度上取得了突破，但一个核心问题仍然未得到解决：如何在大比例放大时恢复更精细的纹理细节？基于优化的超分辨率方法的行为主要由目标函数的选择驱动。最近的研究主要集中在最小化均方重建误差上。虽然由此得到的估计具有较高的峰值信噪比（PSNR），但它们通常缺乏高频细节，并且在感知上不令人满意，因为它们无法达到高分辨率下预期的保真度。本文提出了 SRGAN，一种用于图像超分辨率的生成对抗网络（GAN）。据我们所知，这是第一个能够推断出 4 倍放大因子的逼真自然图像的框架。为了实现这一点，我们提出了一个感知损失函数，该函数由对抗损失和内容损失组成。对抗损失通过一个判别器网络将我们的解推向自然图像流形，该判别器网络被训练用于区分超分辨率图像和原始逼真图像。此外，我们使用基于感知相似性的内容损失，而不是像素空间中的相似性。我们的深度残差网络能够从公共基准测试中的严重下采样图像中恢复逼真的纹理。广泛的平均意见得分（MOS）测试表明，使用 SRGAN 在感知质量上取得了显著提升。SRGAN 获得的 MOS 得分更接近原始高分辨率图像，而不是任何现有最先进方法的结果。

# 2. Introduction
从低分辨率（LR）图像估计高分辨率（HR）图像的任务被称为超分辨率（SR）。SR 在计算机视觉研究社区中受到了广泛关注，并具有广泛的应用。SR 问题的病态性在高放大因子下尤为明显，重建的 SR 图像通常缺乏纹理细节。监督 SR 算法的优化目标通常是最小化恢复的 HR 图像与真实值之间的均方误差（MSE）。这很方便，因为最小化 MSE 也最大化了峰值信噪比（PSNR），这是用于评估和比较 SR 算法的常用指标。然而，MSE（和 PSNR）捕捉感知相关差异（如高纹理细节）的能力非常有限，因为它们是基于像素级图像差异定义的。本文提出了一种超分辨率生成对抗网络（SRGAN），我们采用了一个带有跳跃连接的深度残差网络（ResNet），并摒弃了 MSE 作为唯一的优化目标。与之前的工作不同，我们定义了一种新的感知损失，使用 VGG 网络的高级特征图结合一个判别器，鼓励解在感知上难以与 HR 参考图像区分。图 1 展示了一个通过 4 倍放大因子超分辨的逼真图像示例。

## 2.1 相关工作
### 2.1.1 图像超分辨率
最近的图像 SR 综述文章包括 Nasrollahi 和 Moeslund [43] 或 Yang 等人 [61]。这里我们将专注于单图像超分辨率（SISR），不再讨论从多图像恢复 HR 图像的方法 [4, 15]。
- 预测方法是最早解决 SISR 的方法之一。虽然这些滤波方法（如线性、双三次或 Lanczos [14] 滤波）可以非常快，但它们过度简化了 SISR 问题，通常会产生过于平滑的纹理解。
- 特别关注边缘保留的方法已被提出 [1, 39]。更强大的方法旨在建立低分辨率和高分辨率图像信息之间的复杂映射，通常依赖于训练数据。
- 许多基于示例对的方法依赖于已知对应 HR 对应物的 LR 训练 patch。早期的工作由 Freeman 等人 [18, 17] 提出。
- 与 SR 问题相关的方法起源于压缩感知 [62, 12, 69]。在 Glasner 等人 [21] 中，作者利用图像跨尺度的 patch 冗余来驱动 SR。这种自相似性范式也在 Huang 等人 [31] 中采用，其中自字典通过允许小的变换和形状变化得到扩展。Gu 等人 [25] 提出了一种卷积稀疏编码方法，通过处理整个图像而不是重叠的 patch 来提高一致性。为了在避免边缘伪影的同时重建逼真的纹理细节，Tai 等人 [52] 结合了基于梯度轮廓先验 [50] 的边缘导向 SR 算法和学习型细节合成的优势。Zhang 等人 [70] 提出了一个多尺度字典，以捕捉不同尺度下相似图像 patch 的冗余。为了超分辨地标图像，Yue 等人 [67] 从网络上检索具有相似内容的相关 HR 图像，并提出了一种结构感知匹配准则进行对齐。邻域嵌入方法通过在低维流形中找到相似的 LR 训练 patch 并组合其对应的 HR patch 来上采样 LR 图像 patch [54, 55]。在 Kim 和 Kwon [35] 中，作者强调了邻域方法过拟合的倾向，并使用核岭回归制定了更一般的示例对映射。回归问题也可以通过高斯过程回归 [27]、树 [46] 或随机森林 [47] 来解决。在 Dai 等人 [6] 中，学习了多个 patch 特定的回归器，并在测试期间选择最合适的回归器。
- 最近，基于卷积神经网络（CNN）的 SR 算法表现出色。在 Wang 等人 [59] 中，作者将稀疏表示先验编码到基于学习迭代收缩和阈值算法（LISTA）[23] 的前馈网络架构中。Dong 等人 [9, 10] 使用双三次插值对输入图像进行上采样，并训练了一个三层深度全卷积网络，端到端地实现了最先进的 SR 性能。随后，研究表明，使网络直接学习上采样滤波器可以进一步提高准确性和速度 [11, 48, 57]。Kim 等人 [34] 提出了一个高度性能的深度递归卷积网络（DRCN），该网络允许长距离像素依赖，同时保持模型参数数量较少。与本文特别相关的是 Johnson 等人 [33] 和 Bruna 等人 [5] 的工作，他们依赖于更接近感知相似性的损失函数来恢复视觉上更令人信服的 HR 图像。

### 2.1.2 卷积神经网络的设计
随着 Krizhevsky 等人 [37] 的成功，许多计算机视觉问题的最新技术由专门设计的 CNN 架构设定。研究表明，更深的网络架构可能难以训练，但有可能显著提高网络的准确性，因为它们允许建模非常复杂的映射 [49, 51]。为了有效训练这些更深的网络架构，通常使用批量归一化 [32] 来抵消内部协变量偏移。更深的网络架构也被证明可以提高 SISR 的性能，例如 Kim 等人 [34] 提出了一个递归 CNN 并展示了最先进的结果。另一个强大的设计选择是最近引入的残差块 [29] 和跳跃连接 [30, 34] 的概念。跳跃连接减轻了网络架构对自然中微不足道的恒等映射的建模负担，然而，用卷积核表示可能并非微不足道。在 SISR 的背景下，还表明学习上采样滤波器在准确性和速度方面是有益的 [11, 48, 57]。这是对 Dong 等人 [10] 的改进，他们在将图像输入 CNN 之前使用双三次插值对 LR 观测进行上采样。

### 2.1.3 损失函数
像素级损失函数（如 MSE）难以处理恢复丢失的高频细节（如纹理）时固有的不确定性：最小化 MSE 鼓励找到可能解的平均值，这些解通常过于平滑，因此感知质量较差 [42, 33, 13, 5]。图 2 展示了具有相应 PSNR 的不同感知质量的重建。我们在图 3 中说明了最小化 MSE 的问题，其中多个具有高纹理细节的潜在解被平均以创建平滑的重建。在 Mathieu 等人 [42] 和 Denton 等人 [7] 中，作者通过使用生成对抗网络（GANs）[22] 进行图像生成来解决这个问题。Yu 和 Porikli [66] 通过增加判别器损失来训练一个网络，该网络以大的放大因子（8 倍）超分辨人脸图像。GANs 也被用于无监督表示学习 [44]。Li 和 Wand [38] 描述了使用 GANs 学习从一个流形到另一个流形的映射，用于风格迁移，Yeh 等人 [64] 用于修复。Bruna 等人 [5] 最小化了 VGG 19 [49] 和散射网络特征空间中的平方误差。Dosovitskiy 和 Brox [13] 使用基于在神经网络特征空间中计算的欧几里得距离的损失函数，并结合对抗训练。研究表明，所提出的损失允许视觉上更优的图像生成，并可用于解决解码非线性特征表示的病态逆问题。与这项工作类似，Johnson 等人 [33] 和 Bruna 等人 [5] 建议使用从预训练的 VGG 网络中提取的特征，而不是低级像素级误差度量。具体来说，作者制定了一个基于从 VGG 19 [49] 网络中提取的特征图之间的欧几里得距离的损失函数。在超分辨率和艺术风格迁移 [19, 20] 中获得了感知上更令人信服的结果。最近，Li 和 Wand [38] 还研究了在像素或 VGG 特征空间中比较和混合 patch 的效果。

## 2.2 贡献
GANs 提供了一个强大的框架，用于生成具有高感知质量的逼真自然图像。GAN 过程鼓励重建向包含逼真图像的高概率搜索空间区域移动，从而更接近自然图像流形，如图 3 所示。在本文中，我们描述了第一个使用 GANs 概念形成感知损失函数的非常深的 ResNet [29, 30] 架构，用于逼真的 SISR。我们的主要贡献是：
- 我们通过优化 MSE 的 16 块深度 ResNet（SRResNet）为高放大因子（4 倍）的图像 SR 设定了新的最先进技术，通过 PSNR 和结构相似性（SSIM）衡量。
- 我们提出了 SRGAN，这是一个基于 GAN 的网络，优化了新的感知损失。在这里，我们用基于 VGG 网络 [49] 特征图计算的损失替换了基于 MSE 的内容损失，这些特征图对像素空间的变化更不敏感 [38]。
- 我们通过对三个公共基准数据集上的图像进行广泛的平均意见得分（MOS）测试，确认 SRGAN 是新的最先进技术，以较大的优势估计高放大因子（4 倍）的逼真 SR 图像。

我们在第 2 节中描述了网络架构和感知损失。第 3 节提供了在公共基准数据集上的定量评估以及视觉示例。论文在第 4 节中进行了讨论，并在第 5 节中得出了结论。

# 3. 方法
在 SISR 中，目标是从低分辨率输入图像 $I_{LR}$ 估计高分辨率超分辨率图像 $I_{SR}$。这里 $I_{LR}$ 是其高分辨率对应物 $I_{HR}$ 的低分辨率版本。高分辨率图像仅在训练期间可用。在训练中，$I_{LR}$ 是通过对 $I_{HR}$ 应用高斯滤波器，然后进行下采样操作得到的，下采样因子为 $r$。对于具有 $C$ 个颜色通道的图像，我们通过大小为 $W \times H \times C$ 的实值张量描述 $I_{LR}$，$I_{HR}$ 和 $I_{SR}$ 分别通过 $rW \times rH \times C$ 描述。我们的最终目标是训练一个生成函数 $G$，该函数估计给定 LR 输入图像对应的 HR 对应物。为了实现这一点，我们训练一个生成器网络作为前馈 CNN $G_{\theta_G}$，参数化为 $\theta_G$。这里 $\theta_G = \{W_{1:L}; b_{1:L}\}$ 表示 $L$ 层深度网络的权重和偏置，并通过优化 SR 特定的损失函数 $l_{SR}$ 获得。对于训练图像 $I_{HR}^n, n=1,...,N$ 和对应的 $I_{LR}^n, n=1,...,N$，我们求解：
$$
\hat{\theta}_G = \arg \min_{\theta_G} \frac{1}{N} \sum_{n=1}^N l_{SR}(G_{\theta_G}(I_{LR}^n), I_{HR}^n)
$$
在这项工作中，我们将专门设计一个感知损失 $l_{SR}$，作为几个损失组件的加权组合，这些组件模拟了恢复的 SR 图像的不同理想特性。各个损失函数在第 2.2 节中详细描述。
![[3-01 SRGAN-001.png]]

## 3.1 对抗网络架构
根据 Goodfellow 等人 [22]，我们进一步定义了一个判别器网络 $D_{\theta_D}$，我们与 $G_{\theta_G}$ 交替优化，以解决对抗性最小-最大问题：
$$
\min_{\theta_G} \max_{\theta_D} \mathbb{E}_{I_{HR} \sim p_{train}(I_{HR})} [\log D_{\theta_D}(I_{HR})] + \mathbb{E}_{I_{LR} \sim p_G(I_{LR})} [\log(1 - D_{\theta_D}(G_{\theta_G}(I_{LR})))]
$$
**这种公式背后的总体思想是，它允许训练一个生成模型 $G$，目标是欺骗一个可微分的判别器 $D$，该判别器被训练用于区分超分辨率图像和真实图像。** 通过这种方法，我们的生成器可以学习创建与真实图像高度相似的解，因此难以被 $D$ 分类。这鼓励了位于自然图像流形子空间中的感知上更优的解。这与通过最小化像素级误差测量（如 MSE）获得的 SR 解形成对比。
在我们非常深的生成器网络 $G$ 的核心，如图 4 所示，是 $B$ 个具有相同布局的残差块。受 Johnson 等人 [33] 的启发，我们采用了 Gross 和 Wilber [24] 提出的块布局。具体来说，我们使用两个具有小 $3 \times 3$ 核和 64 个特征图的卷积层，然后是批量归一化层 [32] 和 ParametricReLU [28] 作为激活函数。我们通过两个训练的子像素卷积层增加输入图像的分辨率，如 Shi 等人 [48] 所提出的。为了区分真实的 HR 图像和生成的 SR 样本，我们训练了一个判别器网络。架构如图 4 所示。
我们遵循 Radford 等人 [44] 总结的架构指南，使用 LeakyReLU 激活（$\alpha = 0.2$），并在整个网络中避免最大池化。判别器网络被训练来解决公式 2 中的最大化问题。它包含八个卷积层，具有越来越多的 $3 \times 3$ 滤波器核，从 64 个核增加到 512 个核，如 VGG 网络 [49] 中所示。每次特征数量翻倍时，使用步幅卷积来减少图像分辨率。生成的 512 个特征图后跟两个密集层和一个最终的 sigmoid 激活函数，以获得样本分类的概率。

## 3.2 感知损失函数
我们的感知损失函数 $l_{SR}$ 的定义对于生成器网络的性能至关重要。虽然 $l_{SR}$ 通常基于 MSE [10, 48] 建模，但我们改进了 Johnson 等人 [33] 和 Bruna 等人 [5]，设计了一个评估解在感知相关特性方面的损失函数。我们将感知损失公式化为内容损失（$l_{SR}^X$）和对抗损失组件的加权和：
$$
l_{SR} = \underbrace{l_{SR}^X}_{\text{内容损失}} + 10^{-3} \underbrace{l_{SR}^{Gen}}_{\text{对抗损失}} \quad \text{（对于基于 VGG 的内容损失）}
$$
在下面，我们描述了内容损失 $l_{SR}^X$ 和对抗损失 $l_{SR}^{Gen}$ 的可能选择。

### 3.2.1 内容损失
像素级 MSE 损失计算为：
$$
l_{SR}^{MSE} = \frac{1}{r^2 WH} \sum_{x=1}^{rW} \sum_{y=1}^{rH} (I_{HR}^{x,y} - G_{\theta_G}(I_{LR})^{x,y})^2
$$
这是图像 SR 最广泛使用的优化目标，许多最先进的方法都依赖于它 [10, 48]。然而，虽然 MSE 优化问题的解通常具有较高的 PSNR，但它们通常缺乏高频内容，导致感知上不令人满意的解，具有过于平滑的纹理（参见图 2）。**我们不再依赖于像素级损失，而是基于 Gatys 等人 [19]、Bruna 等人 [5] 和 Johnson 等人 [33] 的思想，使用更接近感知相似性的损失函数。** 我们基于预训练的 19 层 VGG 网络的 ReLU 激活层定义了 VGG 损失 [49]。我们用 $\phi_{i,j}$ 表示在 VGG 19 网络中第 $i$ 个最大池化层之前（激活后）的第 $j$ 个卷积获得的特征图，我们认为这是给定的。然后，我们将 VGG 损失定义为重建图像 $G_{\theta_G}(I_{LR})$ 和参考图像 $I_{HR}$ 的特征表示之间的欧几里得距离：
$$
l_{SR}^{VGG/i,j} = \frac{1}{W_{i,j} H_{i,j}} \sum_{x=1}^{W_{i,j}} \sum_{y=1}^{H_{i,j}} (\phi_{i,j}(I_{HR})^{x,y} - \phi_{i,j}(G_{\theta_G}(I_{LR}))^{x,y})^2
$$
这里 $W_{i,j}$ 和 $H_{i,j}$ 描述了 VGG 网络中相应特征图的维度。

什么是 VGG 损失
### 核心原理
1. **VGG 网络基础**：
   - VGG 是经典的深度卷积神经网络，在 ImageNet 数据集上预训练，擅长提取图像的语义特征。
   - 其浅层捕捉边缘、纹理等低级特征，深层捕捉物体、结构等高级特征。
2. **感知损失（Perceptual Loss）**：
   - 传统损失函数（如均方误差 MSE）仅比较像素级差异，无法反映人类感知的相似性。
   - 感知损失通过比较生成图像与目标图像在预训练网络特征空间中的距离，更关注语义层面的相似性。

3. **VGG 损失的定义**：
   - ==从 VGG 网络的某一层（如 `conv3_3`）提取特征图==。
   - 计算生成图像和目标图像特征图之间的均方误差（MSE）或余弦相似度作为损失值。
	$$\mathcal{L}_{\text{VGG}} = \frac{1}{C H W} \sum_{c, h, w} \left ( \phi_{\text{VGG}}(I_{\text{gen}})^{(c, h, w)} - \phi_{\text{VGG}}(I_{\text{target}})^{(c, h, w)} \right)^2$$
   - 其中：
     - $\phi_{\text{VGG}}$：VGG 网络的特征提取函数（如某层输出）。
     - $I_{\text{gen}}$、$I_{\text{target}}$：生成图像和目标图像。
     - $C, H, W$：特征图的通道数、高度和宽度。

### 3.2.2 对抗损失
除了上述内容损失外，我们还将 GAN 的生成组件添加到感知损失中。这鼓励我们的网络倾向于位于自然图像流形上的解，通过试图欺骗判别器网络。生成损失 $l_{SR}^{Gen}$ 基于判别器 $D_{\theta_D}(G_{\theta_G}(I_{LR}))$ 对所有训练样本的概率定义为：
$$
l_{SR}^{Gen} = \sum_{n=1}^N -\log D_{\theta_D}(G_{\theta_G}(I_{LR}))
$$
这里，$D_{\theta_D}(G_{\theta_G}(I_{LR}))$ 是重建图像 $G_{\theta_G}(I_{LR})$ 是自然 HR 图像的概率。为了更好的梯度行为，我们最小化 $-\log D_{\theta_D}(G_{\theta_G}(I_{LR}))$ 而不是 $\log[1 - D_{\theta_D}(G_{\theta_G}(I_{LR}))]$ [22]。

# 4. 实验
## 4.1 数据和相似性度量
我们在三个广泛使用的基准数据集 Set 5 [3]、Set 14 [69] 和 BSD 100（BSD 300 [41] 的测试集）上进行了实验。所有实验都是在低分辨率和高分辨率图像之间 4 倍放大因子的情况下进行的。这对应于图像像素的 16 倍减少。为了公平比较，所有报告的 PSNR [### 4. 实验（续）
## 4.1 数据和相似性度量（续）
所有报告的 PSNR [dB] 和 SSIM [58] 度量均使用 daala 包在中心裁剪的图像（从每个边界去除 4 像素宽的条带）的 y 通道上计算。超分辨率图像的结果包括最近邻、双三次、SRCNN [9] 和 SelfExSR [31]，这些结果从 Huang 等人 [31] 的在线材料中获取，DRCN 的结果从 Kim 等人 [34] 获取。使用 SRResNet（损失函数为 $l_{SR}^{MSE}$ 和 $l_{SR}^{VGG/2.2}$）和 SRGAN 变体的结果可在网上获取。统计测试采用配对双样本 Wilcoxon 符号秩检验，显著性水平为 $p < 0.05$。

读者可能还对 GitHub 上独立开发的基于 GAN 的解决方案感兴趣。然而，它仅提供了在有限人脸集上的实验结果，这是一个更受约束且更容易的任务。

## 4.2 训练细节和参数
我们在 NVIDIA Tesla M 40 GPU 上训练了所有网络，使用了从 ImageNet 数据库 [45] 中随机抽取的 35 万张图像。这些图像与测试图像不同。我们通过对 HR 图像（BGR，$C=3$）使用双三次核进行下采样，下采样因子为 $r=4$，得到 LR 图像。对于每个 mini-batch，我们从不同的训练图像中随机裁剪 16 个 $96 \times 96$ 的 HR 子图像。请注意，我们可以将生成器模型应用于任意大小的图像，因为它是全卷积的。我们将 LR 输入图像的范围缩放到 $[0,1]$，HR 图像的范围缩放到 $[-1,1]$。因此，MSE 损失是在强度范围为 $[-1,1]$ 的图像上计算的。VGG 特征图也通过因子 $\frac{1}{12.75}$ 进行缩放，以获得与 MSE 损失相当的 VGG 损失。这相当于将公式 5 乘以约 0.006 的缩放因子。我们使用 Adam [36] 进行优化，$\beta_1 = 0.9$。SRResNet 网络以学习率 $10^{-4}$ 和 $10^6$ 次更新迭代进行训练。我们在训练实际的 GAN 时，使用训练好的基于 MSE 的 SRResNet 网络作为生成器的初始化，以避免不希望的局部最优。所有 SRGAN 变体以学习率 $10^{-4}$ 进行 $10^5$ 次更新迭代，然后以较低的学习率 $10^{-5}$ 进行另外 $10^5$ 次迭代。我们交替更新生成器和判别器网络，这相当于 Goodfellow 等人 [22] 中使用的 $k=1$。我们的生成器网络有 16 个相同的残差块（$B=16$）。在测试时，我们关闭批量归一化更新，以获得仅依赖于输入的确定性输出 [32]。我们的实现基于 Theano [53] 和 Lasagne [8]。

## 4.3 平均意见得分（MOS）测试
我们进行了 MOS 测试，以量化不同方法重建感知上令人信服的图像的能力。具体来说，我们要求 26 名评分者对超分辨率图像进行评分，评分范围为 1（质量差）到 5（质量优秀）。评分者对 Set 5、Set 14 和 BSD 100 上的超分辨率图像进行评分。在 BSD 100 上，每个图像的 9 个版本由每个评分者评分。在 Set 5 和 Set 14 上，评分者还评定了所提出方法的三个额外版本，以研究不同的内容损失。总共获得了 26*100*9 + 26*14*12 + 26*5*12 = 29328 个评分，每个评分者评定了 1128 张图像。图像以完全随机的方式呈现，没有任何关于所使用超分辨率方法的提示。评分者在不包括在测试集中的图像上进行校准，使得最近邻插值的重建应得分为 1（差），原始高分辨率图像得分为 5（优秀）。每个数据集上的 MOS 评分分布总结在图 9 中。所有相应评分的平均序数排名如图 10 所示。请注意，1 分对应于最佳排名，对于具有相同序数排名的样本，排名是平均的。虽然由于样本量非常小且图像细节较少，Set 5 上的结果有些不确定，但 Set 14 和特别是大型 BSD 100 数据集上的评分证实，SRGAN 显著优于任何比较的最先进方法。事实上，使用 SRGAN 获得的 MOS 评分更接近原始高分辨率图像，而不是任何参考方法的结果。

## 4.4 内容损失的研究
我们研究了在 GAN 网络中不同内容损失选择对感知损失的影响。具体来说，我们研究了 $l_{SR} = l_{SR}^X + 10^{-3} l_{SR}^{Gen}$ 对于以下内容损失 $l_{SR}^X$ 的影响：
- **SRGAN-MSE**：$l_{SR}^{MSE}$，研究使用标准 MSE 作为内容损失的对抗网络。
- **SRGAN-VGG 22**：$l_{SR}^{VGG/2.2}$，使用 $\phi_{2,2}$，这是一个在表示低级特征的特征图上定义的损失 [68]。
- **SRGAN-VGG 54**：$l_{SR}^{VGG/5.4}$，使用 $\phi_{5,4}$，这是一个在更深网络层中表示更高级特征的特征图上定义的损失 [68, 65, 40]。我们将其称为 SRGAN。

我们还评估了生成器网络在没有对抗组件的情况下对于两个损失 $l_{SR}^{MSE}$（SRResNet-MSE）和 $l_{SR}^{VGG/2.2}$（SRResNet-VGG 22）的性能。我们将 SRResNet-MSE 称为 SRResNet。请注意，在训练 SRResNet-VGG 22 时，我们向 $l_{SR}^{VGG/2.2}$ 添加了额外的总变差损失，权重为 $2 \times 10^{-8}$ [2, 33]。定量结果总结在表 1 中，视觉示例如图 6 所示。即使与对抗损失结合，MSE 提供的解具有最高的 PSNR 值，然而，这些解在感知上过于平滑，不如使用对视觉感知更敏感的损失组件获得的结果令人信服。这是由于基于 MSE 的内容损失和对抗损失之间的竞争所致。我们还将少数 SRGAN-MSE 重建中观察到的轻微重建伪影归因于这些竞争目标。我们无法确定 SRResNet 或 SRGAN 在 Set 5 上的最佳损失函数。然而，SRGAN-VGG 54 在 Set 14 上的 MOS 评分显著优于其他 SRGAN 和 SRResNet 变体。我们观察到，使用更高级的 VGG 特征图 $\phi_{5,4}$ 与 $\phi_{2,2}$ 相比，能够产生更好的纹理细节（参见图 6）。通过 SRGAN 相对于 SRResNet 的感知改进的更多示例在补充材料中提供。

## 4.5 最终网络的性能
我们比较了 SRResNet 和 SRGAN 与最近邻、双三次插值和四种最先进方法的性能。定量结果总结在表 2 中，并确认 SRResNet（在 PSNR/SSIM 方面）在三个基准数据集上设定了新的最先进技术。请注意，我们使用了公开可用的框架进行评估（参见第 3.1 节），因此报告的值可能与原始论文中报告的值略有不同。我们进一步获得了 SRGAN 和所有参考方法在 BSD 100 上的 MOS 评分。使用 SRResNet 和 SRGAN 超分辨的图像示例如补充材料所示。表 2 中的结果证实，SRGAN 以较大的优势优于所有参考方法，并为逼真图像 SR 设定了新的最先进技术。BSD 100 上的所有 MOS 差异（参见表 2）均高度显著，除了 SRCNN 与 SelfExSR 之间的差异。所有收集的 MOS 评分的分布总结在图 5 中。

# 5. 讨论与未来工作
我们通过 MOS 测试确认了 SRGAN 的优越感知性能。我们还表明，标准的定量度量（如 PSNR 和 SSIM）无法捕捉和准确评估与人类视觉系统相关的图像质量 [56]。这项工作的重点是超分辨率图像的感知质量，而不是计算效率。与 Shi 等人 [48] 相比，所提出的模型并未针对实时视频 SR 进行优化。然而，对网络架构的初步实验表明，较浅的网络有可能在质量性能略有降低的情况下提供非常高效的替代方案。与 Dong 等人 [10] 相比，我们发现更深的网络架构是有益的。我们推测 ResNet 设计对更深网络的性能有重大影响。我们发现，即使更深的网络（$B > 16$）也可以进一步提高 SRResNet 的性能，然而，这以更长的训练和测试时间为代价（参见补充材料）。我们还发现，更深的 SRGAN 变体由于高频伪影的出现而越来越难以训练。

在追求 SR 问题的逼真解时，内容损失的选择尤为重要，如图 6 所示。在这项工作中，我们发现 $l_{SR}^{VGG/5.4}$ 能够产生感知上最令人信服的结果，我们将其归因于更深网络层表示更高抽象特征的潜力 [68, 65, 40]，远离像素空间。我们推测，这些更深层的特征图纯粹关注内容，而对抗损失则专注于纹理细节，这些细节是没有对抗损失的超分辨率图像与逼真图像之间的主要区别。我们还注意到，理想的损失函数取决于应用。例如，幻觉更精细细节的方法可能不太适合医疗应用或监控。文本或结构化场景 [31] 的感知上令人信服的重建具有挑战性，是未来工作的一部分。开发描述图像空间内容但对像素空间变化更不敏感的内容损失函数将进一步改善逼真图像 SR 结果。

# 6. 结论
我们描述了一个深度残差网络 SRResNet，它在使用广泛使用的 PSNR 度量评估的公共基准数据集上设定了新的最先进技术。我们强调了这种以 PSNR 为重点的图像超分辨率的一些局限性，并引入了 SRGAN，它通过训练 GAN 增强了内容损失函数的对抗损失。通过广泛的 MOS 测试，我们确认，SRGAN 在大放大因子（4 倍）下的重建比使用最先进参考方法获得的重建在逼真度上显著更好。
