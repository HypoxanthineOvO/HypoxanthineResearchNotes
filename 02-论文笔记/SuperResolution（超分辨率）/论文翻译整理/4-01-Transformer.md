# 1. Abstract
![[4-01-Transformer-024.png]]
本文研究了图像超分辨率（Image Super-Resolution, SR）任务，旨在从低分辨率（Low-Resolution, LR）图像中恢复逼真的纹理。最近的进展是通过将高分辨率（High-Resolution, HR）图像作为参考（Ref），从而将相关纹理转移到 LR 图像中。然而，现有的 SR 方法忽略了使用注意力机制从 Ref 图像中转移 HR 纹理，这限制了这些方法在复杂情况下的表现。本文提出了一种新颖的 **Texture Transformer Network for Image Super-Resolution (TTSR)**，其中 LR 和 Ref 图像分别被表示为 transformer 中的查询（query）和键（key）。TTSR 由四个紧密相关的模块组成，这些模块针对图像生成任务进行了优化，包括一个可学习的纹理提取器（DNN）、一个相关性嵌入模块、一个用于纹理转移的硬注意力模块和一个用于纹理合成的软注意力模块。这种设计鼓励了 LR 和 Ref 图像之间的联合特征学习，通过注意力机制可以发现深层次的特征对应关系，从而准确地转移纹理特征。所提出的纹理 transformer 可以进一步以跨尺度方式堆叠，从而能够从不同级别（例如，从 1 倍到 4 倍放大）恢复纹理。大量实验表明，TTSR 在定量和定性评估上均显著优于现有最先进的方法。

# 2. Introduction
图像超分辨率旨在从其退化的低分辨率对应图像中恢复自然且逼真的纹理。近年来，图像 SR 的成功极大地提升了媒体内容的质量，从而改善了用户体验。例如，移动相机的数字变焦算法和数字电视的图像增强技术。此外，这项基础技术还可以受益于广泛的计算机视觉任务，如医学成像和卫星成像。图像 SR 的研究通常基于两种范式：单图像超分辨率（Single Image Super-Resolution, SISR）和基于参考的图像超分辨率（Reference-based Image Super-Resolution, RefSR）。传统的 SISR 通常会产生模糊效果，因为在退化过程中 HR 纹理被过度破坏，无法恢复。尽管基于生成对抗网络（GANs）的图像 SR 方法被提出以缓解上述问题，但 GANs 引起的幻觉和伪影进一步给图像 SR 任务带来了巨大挑战。
**所遇到的现有方法的主要问题**：现有的 RefSR 方法通常采用直接的方式转移纹理，忽略了使用注意力机制来从参考图像中提取和转移 HR 纹理。这导致在复杂情况下（如大视角变化时），这些方法可能无法准确转移纹理，生成不满意的 SR 图像。例如，Zheng 等人提出的基于光流的方法在面对 LR 和 Ref 图像之间的大视角变化时，可能会搜索和转移不准确的纹理。Zhang 等人使用预训练分类模型的特征空间进行纹理搜索和转移，但这些高层语义特征无法有效表示 HR 纹理，导致生成的结果仍然不理想。
**我们提出的方法概述**：为了解决这些问题，本文提出了一种新颖的 **Texture Transformer Network for Image Super-Resolution (TTSR)**。TTSR 通过引入 transformer 架构，将 LR 和 Ref 图像分别表示为查询（query）和键（key），并设计了四个紧密相关的模块：可学习的纹理提取器（LTE）、相关性嵌入模块、硬注意力模块（HA）和软注意力模块（SA）。这些模块共同作用，实现了从 Ref 图像中准确转移 HR 纹理到 LR 图像，从而生成更逼真的 SR 图像。

# 3. Related Work
## 3.1 Single Image Super-Resolution
近年来，基于深度学习的 SISR 方法在传统非学习方法的基础上取得了显著进展。**基于深度学习的 SISR 方法将这一问题视为密集图像回归任务，通过学习由 CNN 表示的 LR 和 HR 图像之间的端到端图像映射函数。**
- Dong 等人提出了 SRCNN，首次将深度学习引入 SISR，使用三层 CNN 表示映射函数。
- Dong 等人进一步通过用原始 LR 图像替换插值后的 LR 图像，并在最后一层使用反卷积来放大特征图，从而加速了 SR 过程。
- 随后，Kim 等人提出了 VDSR 和 DRCN，使用更深网络进行残差学习。
- Shi 等人用子像素卷积层替换反卷积，以减少棋盘伪影。
- 残差块被引入 SISR 中的 SRResNet，并在 EDSR 中得到了改进。在残差块的帮助下，许多工作专注于设计更深或更宽的网络。
- Zhang 等人和 Tong 等人采用密集块来结合不同层次的特征。
- Zhang 等人通过添加通道注意力改进了残差块。Liu 等人提出了一个非局部循环网络用于图像恢复。
- Dai 等人引入了二阶统计量以获得更具判别性的特征表示。
上述方法使用均方误差（MSE）或平均绝对误差（MAE）作为其目标函数，忽略了人类感知。近年来，越来越多的研究旨在提高感知质量：
- Johnson 等人将感知损失引入 SR 任务，而 SRGAN 采用生成对抗网络（GANs）并展示了视觉上令人满意的结果。
- Sajjadi 等人使用基于 Gram 矩阵的纹理匹配损失来强制执行局部相似纹理
- ESRGAN 通过引入 RRDB 和相对对抗损失增强了 SRGAN。
- 最近提出的 RSRGAN 训练了一个排序器，并使用排序内容损失来优化感知质量，取得了最先进的视觉结果。

## 3.2 Reference-based Image Super-Resolution
与 SISR 不同，**RefSR 可以从 Ref 图像中获取更准确的细节**。这可以通过图像对齐或 patch 匹配等方法实现。一些现有的 RefSR 方法选择对齐 LR 和 Ref 图像。Landmark 通过全局注册将 Ref 图像对齐到 LR 图像，以解决能量最小化问题。Wang 等人通过在特征合成之前反复应用非均匀变形来增强 Ref 图像。CrossNet 采用光流在不同尺度上对齐 LR 和 Ref 图像，并将它们连接到解码器的相应层中。然而，这些方法的性能在很大程度上取决于 LR 和 Ref 图像之间的对齐质量。此外，光流等对齐方法耗时，不利于实际应用。其他 RefSR 方法采用“patch match”方法来搜索适当的参考信息。Boominathan 等人匹配了 LR 图像和下采样 Ref 图像的梯度特征之间的 patch。Zheng 等人用卷积神经网络中的特征替换了简单的梯度特征，以应用语义匹配，并使用 SISR 方法进行特征合成。最近的工作 SRNTT 在 LR 和 Ref 图像的 VGG 特征之间应用 patch 匹配以交换相似的纹理特征。然而，SRNTT 忽略了原始特征和交换特征之间的相关性，并将所有交换特征平等地输入到主网络中。为了解决这些问题，我们提出了一种纹理 transformer 网络，使我们的方法能够从 Ref 到 LR 图像中搜索和转移相关纹理。此外，通过提出的跨尺度特征集成模块堆叠多个纹理 transformer，可以进一步提高我们方法的性能。

# 4. Approach
![[4-01-Transformer-025.png|600x794]]
## 4.1 Texture Transformer
纹理 transformer 的结构如图 2 所示。LR、LR↑和 Ref 分别表示输入图像、4 倍双三次上采样的输入图像和参考图像。我们依次对 Ref 应用双三次下采样和上采样，得到与 LR↑域一致的 Ref↓↑。纹理 transformer 将 Ref、Ref↓↑、LR↑和骨干网络生成的 LR 特征作为输入，并输出一个合成的特征图，该特征图将用于生成 HR 预测。纹理 transformer 包含四个部分：可学习的纹理提取器（LTE）、相关性嵌入模块（RE）、用于特征转移的硬注意力模块（HA）和用于特征合成的软注意力模块（SA）。细节将在下面讨论。

### 4.1.1 Learnable Texture Extractor
在 RefSR 任务中，参考图像的纹理提取是至关重要的，因为准确且适当的纹理信息将有助于生成 SR 图像。与使用预训练分类模型（如 VGG）提取的语义特征不同，我们设计了一个可学习的纹理提取器，其参数将在端到端训练中更新。这种设计鼓励了 LR 和 Ref 图像之间的联合特征学习，从而捕获更准确的纹理特征。纹理提取的过程可以表示为：
$$
Q = LTE(LR↑), \quad K = LTE(Ref↓↑), \quad V = LTE(Ref)
$$
其中 $LTE(·)$ 表示我们可学习纹理提取器的输出。提取的纹理特征 $Q$（查询）、$K$（键）和 $V$（值）表示 transformer 中注意力机制的三个基本元素，并将进一步用于我们的相关性嵌入模块。

### 4.1.2 Relevance Embedding
相关性嵌入旨在通过估计 $Q$ 和 $K$ 之间的相似性来嵌入 LR 和 Ref 图像之间的相关性。我们将 $Q$ 和 $K$ 展开为 patch，分别表示为 $q_i$（$i \in [1, H_{LR} \times W_{LR}]$）和 $k_j$（$j \in [1, H_{Ref} \times W_{Ref}]$）。然后，对于 $Q$ 中的每个 patch $q_i$ 和 $K$ 中的 $k_j$，我们通过归一化内积计算这两个 patch 之间的相关性 $r_{i,j}$：
$$
r_{i,j} = \left\langle \frac{q_i}{\|q_i\|}, \frac{k_j}{\|k_j\|} \right\rangle
$$
相关性进一步用于获得硬注意力图和软注意力图。

### 4.1.3 Hard-Attention
我们提出了一个硬注意力模块，用于从 Ref 图像中转移 HR 纹理特征 $V$。传统的注意力机制对每个查询 $q_i$ 取 $V$ 的加权和。然而，这种操作可能会导致模糊效果，缺乏转移 HR 纹理特征的能力。因此，在我们的硬注意力模块中，我们只从 $V$ 中最相关的位置转移特征到每个查询 $q_i$。更具体地说，我们首先计算一个硬注意力图 $H$，其中第 $i$ 个元素 $h_i$（$i \in [1, H_{LR} \times W_{LR}]$）从相关性 $r_{i,j}$ 中计算得出：
$$
h_i = \arg \max_j r_{i,j}
$$
$h_i$ 的值可以被视为一个硬索引，表示 Ref 图像中与 LR 图像中第 $i$ 个位置最相关的位置。为了从 Ref 图像中获得转移的 HR 纹理特征 $T$，我们使用硬注意力图作为索引，对 $V$ 的展开 patch 应用索引选择操作：
$$
t_i = v_{h_i}
$$
其中 $t_i$ 表示 $T$ 在第 $i$ 个位置的值，该值从 $V$ 的第 $h_i$ 个位置中选择。结果，我们获得了 LR 图像的 HR 特征表示 $T$，该表示将用于我们的软注意力模块。

### 4.1.4 Soft-Attention
我们提出了一个软注意力模块，用于从转移的 HR 纹理特征 $T$ 和从 DNN 骨干网络中提取的 LR 特征 $F$ 中合成特征。在合成过程中，应增强相关纹理转移，同时减少不相关的纹理转移。为了实现这一点，从 $r_{i,j}$ 计算出一个软注意力图 $S$，以表示 $T$ 中每个位置的转移纹理特征的置信度：
$$
s_i = \max_j r_{i,j}
$$
其中 $s_i$ 表示软注意力图 $S$ 的第 $i$ 个位置。我们首先将 HR 纹理特征 $T$ 与 LR 特征 $F$ 融合，以利用 LR 图像中的更多信息。然后，这些融合的特征通过软注意力图 $S$ 进行逐元素乘法，并加回到 $F$ 中，以获得纹理 transformer 的最终输出。该操作可以表示为：
$$
F_{out} = F + Conv(Concat(F, T)) \odot S
$$
其中 $F_{out}$ 表示合成的输出特征。$Conv$ 和 $Concat$ 分别表示卷积层和连接操作。运算符 $\odot$ 表示特征图之间的逐元素乘法。总之，纹理 transformer 可以有效地将 Ref 图像中的相关 HR 纹理特征转移到 LR 特征中，从而促进更准确的纹理生成过程。

## 4.2 Cross-Scale Feature Integration
我们的纹理 transformer 可以进一步以跨尺度方式堆叠，并结合跨尺度特征集成模块。架构如图 3 所示。堆叠的纹理 transformer 输出三个分辨率尺度（1 倍、2 倍和 4 倍）的合成特征，从而可以将不同尺度的纹理特征融合到 LR 图像中。为了在不同尺度之间学习更好的表示，受 [25, 37] 启发，我们提出了一个跨尺度特征集成模块（CSFI），以在不同尺度的特征之间交换信息。每次将 LR 特征上采样到下一个尺度时，都会应用一个 CSFI 模块。对于 CSFI 模块中的每个尺度，它通过上/下采样接收来自其他尺度的交换特征，然后在通道维度上进行连接操作。然后，卷积层将特征映射回原始通道数。在这种设计中，从堆叠的纹理 transformer 转移的纹理特征在每个尺度之间交换，从而实现了更强大的特征表示。这种跨尺度特征集成模块进一步提高了我们方法的性能。

## 4.3 Loss Function
我们的方法中有 3 个损失函数。总体损失可以解释为：
$$
L_{overall} = \lambda_{rec} L_{rec} + \lambda_{adv} L_{adv} + \lambda_{per} L_{per}
$$

### 4.3.1 Reconstruction Loss
第一个损失是重建损失：
$$
L_{rec} = \frac{1}{CHW} \| I_{HR} - I_{SR} \|_1
$$
其中 $(C, H, W)$ 是 HR 的大小。我们使用 $L_1$ 损失，与 $L_2$ 损失相比，它已被证明在性能上更锐利且更容易收敛。

### 4.3.2 Adversarial Loss
生成对抗网络 [7] 被证明在生成清晰且视觉上令人愉悦的图像方面是有效的。在这里，我们采用 WGAN-GP [8]，它提出了梯度范数的惩罚化以替代权重裁剪，从而实现了更稳定的训练和更好的性能。该损失可以解释为：
$$
L_D = \mathbb{E}_{\tilde{x} \sim P_g} [D(\tilde{x})] - \mathbb{E}_{x \sim P_r} [D(x)] + \lambda \mathbb{E}_{\hat{x} \sim P_{\hat{x}}} [(\| \nabla_{\hat{x}} D(\hat{x}) \|_2 - 1)^2]
$$
$$
L_G = -\mathbb{E}_{\tilde{x} \sim P_g} [D(\tilde{x})]
$$

### 4.3.3 Perceptual Loss
感知损失已被证明有助于提高视觉质量，并已在 [13, 16, 22, 41] 中使用。感知损失的关键思想是增强预测图像和目标图像在特征空间中的相似性。在这里，我们的感知损失包含两部分：
$$
L_{per} = \frac{1}{C_i H_i W_i} \| \phi_{vgg}^i (I_{SR}) - \phi_{vgg}^i (I_{HR}) \|_2^2 + \frac{1}{C_j H_j W_j} \| \phi_{lte}^j (I_{SR}) - T \|_2^2
$$
其中第一部分是传统的感知损失，其中 $\phi_{vgg}^i (·)$ 表示 VGG 19 的第 $i$ 层特征图，$(C_i, H_i, W_i)$ 表示该层特征图的形状。$I_{SR}$ 是预测的 SR 图像。我们感知损失中的第二部分是转移感知损失，其中 $\phi_{lte}^j (·)$ 表示从提出的 LTE 的第 $j$ 层提取的纹理特征图，$(C_j, H_j, W_j)$ 表示该层的形状。$T$ 是图 2 中转移的 HR 纹理特征。这种转移感知损失约束了预测的 SR 图像与转移的纹理特征 $T$ 具有相似的纹理特征，从而使我们的方法能够更有效地转移 Ref 纹理。

## 4.4 Implementation Details
可学习的纹理提取器包含 5 个卷积层和 2 个池化层，输出三个不同尺度的纹理特征。为了减少时间和 GPU 内存的消耗，相关性嵌入仅应用于最小尺度，并进一步传播到其他尺度。对于判别器，我们采用与 SRNTT [41] 相同的网络，并删除所有 BN 层。在训练期间，我们通过随机水平和垂直翻转以及随机旋转 90°、180° 和 270° 来增强训练图像。每个 mini-batch 包含 9 个大小为 $40 \times 40$ 的 LR patch 以及 9 个大小为 $160 \times 160$ 的 HR 和 Ref patch。$L_{rec}$、$L_{adv}$ 和 $L_{per}$ 的权重系数分别为 1、1 e-3 和 1 e-2。使用 $\beta_1 = 0.9$、$\beta_2 = 0.999$ 和 $\epsilon = 1e-8$ 的 Adam 优化器，学习率为 1 e-4。我们首先对网络进行 2 个 epoch 的预热，其中仅应用 $L_{rec}$。之后，所有损失都参与训练另外 50 个 epoch。

# 5. Experiments
## 5.1 Datasets and Metrics
为了评估我们的方法，我们在最近提出的 RefSR 数据集 CUFED 5 [41] 上训练和测试我们的模型。CUFED 5 的训练集包含 11,871 对图像，每对图像由一个输入图像和一个参考图像组成。CUFED 5 测试集中有 126 张测试图像，每张图像都附有 4 个不同相似度级别的参考图像。为了评估在 CUFED 5 上训练的 TTSR 的泛化性能，我们还在 Sun 80 [26]、Urban 100 [11] 和 Manga 109 [20] 上测试了 TTSR。Sun 80 包含 80 张自然图像，每张图像都配有几个参考图像。对于 Urban 100，我们使用与 [41] 相同的设置，将其 LR 图像视为参考图像。这种设计使得 Urban 100 中的图像能够进行显式的自相似搜索和转移，因为 Urban 100 中的所有图像都是具有强自相似性的建筑图像。对于同样缺乏参考图像的 Manga 109，我们随机从该数据集中采样 HR 图像作为参考图像。由于该数据集由线条、曲线和平坦的彩色区域构成，这些都是常见的模式。即使随机选择一个 HR Ref 图像，我们的方法仍然可以利用这些常见模式并取得良好的结果。SR 结果在 YCbCr 空间的 Y 通道上通过 PSNR 和 SSIM 进行评估。

## 5.2 Evaluation
为了评估 TTSR 的有效性，我们将我们的模型与其他最先进的 SISR 和 RefSR 方法进行了## 5. Experiments (续)
### 5.2 Evaluation (续)
比较。SISR 方法包括 SRCNN [3]、MDSR [17]、RDN [40]、RCAN [39]、SRGAN [16]、ENet [22]、ESRGAN [32]、RSRGAN [38]，其中 RCAN 近年来在 PSNR 和 SSIM 上取得了最先进的性能。RSRGAN 被认为实现了最先进的视觉质量。至于 RefSR 方法，CrossNet [43] 和 SRNTT [41] 是最近的两个最先进方法，显著优于之前的 RefSR 方法。所有实验均在 LR 和 HR 图像之间 4 倍放大因子的情况下进行。

#### 5.2.1 Quantitative Evaluation
为了公平比较，我们遵循 SRNTT [41] 的设置，在 CUFED 5 训练集上训练所有方法，并在 CUFED 5 测试集、Sun 80、Urban 100 和 Manga 109 数据集上进行测试。对于 SR 方法，有一个事实是，使用对抗损失进行训练通常可以获得更好的视觉质量，但会降低 PSNR 和 SSIM 的数值。因此，我们训练了另一个版本的模型，该模型仅优化重建损失，称为 TTSR-rec，以便在 PSNR 和 SSIM 上进行公平比较。表 1 显示了定量评估结果。红色数字表示最高分，蓝色数字表示第二高分。正如比较结果所示，TTSR-rec 在所有四个测试数据集上显著优于最先进的 SISR 方法和最先进的 RefSR 方法。在旨在通过对抗损失获得更好视觉质量的方法中，我们的模型在 Sun 80 和 Manga 109 数据集上仍然表现最佳。在另外两个数据集 CUFED 5 和 Urban 100 上，我们的模型与最先进的模型相比表现相当。定量比较结果证明了我们提出的 TTSR 相对于最先进 SR 方法的优越性。

#### 5.2.2 Qualitative Evaluation
我们的模型在视觉质量上也取得了最佳表现，如图 5 所示。TTSR 可以从参考图像中转移更准确的 HR 纹理，生成令人满意的结果，如图 5 中的前三个示例所示。即使参考图像与输入图像在全局上不相关，我们的 TTSR 仍然可以从局部区域提取更精细的纹理，并将有效的纹理转移到预测的 SR 结果中，如图 5 中的最后三个示例所示。

为了进一步验证我们方法的优越视觉质量，我们进行了一项用户研究，将 TTSR 与四种最先进的方法进行比较，包括 RCAN [39]、RSRGAN [38]、CrossNet [43] 和 SRNTT [41]。共有 10 名受试者参与了这项用户研究，并在 CUFED 5 测试集上收集了 2,520 票。对于每个比较过程，我们向用户提供两张图像，其中一张是 TTSR 图像。用户被要求选择视觉质量更高的图像。图 4 显示了我们的用户研究结果，其中 Y 轴上的值表示用户更喜欢 TTSR 而不是其他方法的百分比。正如我们所看到的，提出的 TTSR 显著优于其他方法，超过 90% 的用户投票支持我们的方法，这验证了 TTSR 的优越视觉质量。

### 5.3 Ablation Study
在本节中，我们验证了我们方法中不同模块的有效性，包括纹理 transformer、跨尺度特征集成、对抗损失和转移感知损失。此外，我们还讨论了不同参考相似度对 TTSR 的影响。

#### 5.3.1 Texture Transformer
我们的纹理 transformer 主要包含四个部分：可学习的纹理提取器（LTE）、相关性嵌入模块、用于特征转移的硬注意力模块（HA）和用于特征合成的软注意力模块（SA）。消融实验结果如表 2 所示。我们通过仅移除所有 BN 层和 Ref 部分重新实现了 SRNTT [41] 作为我们的“Base”模型。在基线模型的基础上，我们逐步添加了 HA、SA 和 LTE。没有 LTE 的模型使用 VGG 19 特征进行相关性嵌入。正如我们所看到的，当添加 HA 时，PSNR 性能从 26.34 提高到 26.59，这验证了硬注意力模块在特征转移中的有效性。当 SA 参与时，相关纹理特征将在特征合成过程中得到增强，而不相关的特征将被缓解。这进一步将性能提升到 26.81。当用提出的 LTE 替换 VGG 时，PSNR 最终提高到 26.92，这证明了 LTE 中联合特征嵌入的优越性。

为了进一步验证我们 LTE 的有效性，我们使用硬注意力图来转移原始图像。预计更好的特征表示可以从原始图像中转移更准确的纹理。图 6 显示了 SRNTT 中的 VGG 19 和 TTSR 中的 LTE 转移的原始图像。在该图中，TTSR 可以转移更准确的参考纹理并生成全局上令人满意的结果，这进一步证明了我们 LTE 的有效性。

#### 5.3.2 Cross-Scale Feature Integration
在纹理 transformer 的基础上，CSFI 可以进一步实现从不同分辨率尺度（1 倍、2 倍和 4 倍）恢复纹理。我们在表 3 中进行了消融实验。第一行显示了仅使用 TT 的模型性能，而第二行证明了 CSFI 的有效性，它带来了 0.17 的 PSNR 提升。为了验证性能提升不是由参数数量增加带来的，我们将“Base+TT”模型的通道数增加到 80 和 96。正如我们所看到的，“Base+TT (C 80)”几乎没有增长，其参数数量与“Base+TT+CSFI”几乎相同。即使我们将参数数量增加到 9.10 M 以获得“Base+TT (C 96)”模型，仍然存在性能差距。这表明 CSFI 可以以相对较小的参数数量有效地利用参考纹理信息。

#### 5.3.3 Adversarial Loss
为了确保感知质量的提升来自模型设计而不是对抗损失，我们在“Base-rec”、“Base”、TTSR-rec 和 TTSR 之间进行了消融实验，其中 TTSR 可以解释为“Base+TT+CSFI”，“-rec”表示仅使用重建损失进行训练。图 7 显示，即使没有感知损失和对抗损失，TTSR-rec 仍然可以利用 Ref 图像并恢复比“Base-rec”更多的细节。在所有损失启用的情况下，TTSR 实现了最佳的视觉结果。

#### 5.3.4 Transferal Perceptual Loss
转移感知损失约束了 LTE 的特征在预测的 SR 图像和转移的图像 $T$ 之间相似。如图 8 所示，使用该损失能够以更有效的方式转移纹理，从而生成视觉上令人愉悦的结果。此外，该损失还将 TTSR 的定量指标 PSNR 和 SSIM 从 25.20/0.757 提高到 25.53/0.765。

#### 5.3.5 Influence of Different Reference Similarity
为了研究 LR 和 Ref 图像之间的相关性如何影响 TTSR 的结果，我们在 CUFED 5 测试集上进行了实验，该测试集具有不同相关性级别的参考图像。表 4 显示了五个相关性级别的结果，其中“L 1”到“L 4”表示 CUFED 5 测试集提供的参考图像，L 1 是最相关的级别，而 L 4 是最不相关的级别。“LR”表示使用输入图像本身作为参考图像。正如表 4 所示，使用 L 1 作为参考图像的 TTSR 表现最佳。当使用 LR 作为参考图像时，TTSR 仍然优于之前最先进的 RefSR 方法。

# 6. Conclusion
在本文中，我们提出了一种新颖的 **Texture Transformer Network for Image Super-Resolution (TTSR)**，它从 Ref 图像中转移 HR 纹理到 LR 图像。所提出的纹理 transformer 包含一个可学习的纹理提取器，该提取器学习联合特征嵌入以进行进一步的注意力计算，以及两个基于注意力的模块，用于从 Ref 图像中转移 HR 纹理。此外，所提出的纹理 transformer 可以与提出的 CSFI 模块以跨尺度方式堆叠，以学习更强大的特征表示。大量实验证明了我们的 TTSR 在定量和定性评估上相对于最先进方法的优越性能。在未来，我们将进一步扩展所提出的纹理 transformer 以应用于通用图像生成任务。