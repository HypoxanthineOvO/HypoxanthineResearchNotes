# 2.1. VDSR
#### Motivation（动机）
尽管深度卷积神经网络（CNN）在图像超分辨率任务中取得了显著进展，但作者观察到，==随着网络深度的增加，训练变得更加困难==。此外，==现有的方法在处理不同通道的特征时缺乏灵活性==，导致网络的表示能力受限。为了解决这些问题，作者提出了**深度残差通道注意力网络（RCAN）**，通过引入**残差中的残差（RIR）结构**和**通道注意力机制（CA）**，进一步提升超分辨率任务的性能。

#### 改进工作方法
1. **残差中的残差结构（RIR）**：
   - RIR 结构由多个残差组（Residual Group, RG）组成，每个残差组包含多个残差块（Residual Block），并通过长跳跃连接（Long Skip Connection, LSC）和短跳跃连接（Short Skip Connection, SSC）进行信息传递。
   - 这种结构==允许低频信息通过多个跳跃连接绕过主网络，使得主网络能够专注于学习高频信息==，从而缓解了深度网络的训练难度。
2. **通道注意力机制（CA）**：
   - 通道注意力机制通过建模通道间的依赖关系，自适应地重新缩放每个通道的特征。具体来说，CA 机制首先通过全局平均池化（Global Average Pooling）生成通道统计量，然后通过一个门控机制（Sigmoid 函数）生成通道权重，最后将这些权重应用于特征图，以增强网络对重要通道的关注。

   2.1. 全局平均池化（Global Average Pooling, GAP）
	- 输入特征图 $X$ 的尺寸为 $H \times W \times C$，其中 $H$ 和 $W$ 是空间维度，$C$ 是通道数。
	- 对每个通道的特征图进行全局平均池化，生成一个通道统计量 $z$，其尺寸为 $1 \times 1 \times C$。具体公式为：$$Z_c = \frac{1}{H \times W} \sum_{i=1}^H \sum_{j=1}^W x_c (i, j)$$
	  其中，$x_c (i, j)$ 表示第 $c$ 个通道在位置 $(i, j)$ 处的值。
   2.2. 通道降维与升维
	- 为了捕捉通道间的非线性关系，通道统计量 $z$ 首先通过一个降维卷积层（Conv 1 x 1），将通道数从 $C$ 减少到 $C/r$（$r$ 是降维比例，通常取 16），然后通过 ReLU 激活函数。
	- 接着，通过一个升维卷积层（Conv 1 x 1），将通道数从 $C/r$ 恢复到 $C$，并通过 Sigmoid 函数生成通道权重 $s$。具体公式为： $$S = \sigma (W_U \cdot \delta (W_D \cdot z))$$
	  其中，$W_D$ 和 $W_U$ 分别是降维和升维卷积层的权重，$\delta$ 是 ReLU 激活函数，$\sigma$ 是 Sigmoid 函数。
   2.3. 通道特征重缩放
	- 生成的通道权重 $s$ 被应用于输入特征图 $X$，对每个通道的特征图进行自适应缩放。具体公式为：$$\hat{x}_c = s_c \cdot x_c$$
    其中，$\hat{x}_c$ 是重缩放后的特征图，$s_c$ 是第 $c$ 个通道的权重。
3. **残差通道注意力块（RCAB）**：
   - RCAB 将通道注意力机制集成到残差块中，进一步增强了网络的表示能力。每个 RCAB 通过两个卷积层提取残差特征，并通过 CA 机制对残差特征进行自适应缩放。
#### 效果
1. **定量结果**：
   - 在多个标准数据集（如 Set 5、Set 14、B 100、Urban 100 和 Manga 109）上，RCAN 在 PSNR 和 SSIM 指标上均优于现有的最先进方法。特别是在大尺度（如 8 倍）超分辨率任务中，RCAN 的性能提升更为显著。
2. **定性结果**：
   - 视觉对比结果显示，RCAN 能够更好地恢复图像中的细节，减少模糊和伪影，生成的高分辨率图像更加清晰和真实。
3. **模型复杂度**：
   - 尽管 RCAN 的网络深度非常大（超过 400 层），但其参数量相对较少，且在性能和模型复杂度之间取得了良好的平衡。
#### 总结
通过引入**残差中的残差结构**和**通道注意力机制**，RCAN 在图像超分辨率任务中取得了显著的性能提升。实验结果表明，RCAN 不仅在定量指标上优于现有方法，而且在视觉质量上也表现出色，特别是在处理大尺度超分辨率任务时表现尤为突出。

> [!notes] 简单的说就是进一步用深度学习的方法改进了网络，用残差机制让网络能学习到应有的特征，并且用注意力机制（单独学习一个权重网络）提高残差特征的比例。
# 2.2. EDSR/MDSR


# 2.3.  MSRN

# 2.4. MSRN