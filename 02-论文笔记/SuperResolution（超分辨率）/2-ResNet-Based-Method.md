# 2.1. VDSR
## Motivation（动机）
尽管深度卷积神经网络（CNN）在图像超分辨率任务中取得了显著进展，但作者观察到，==随着网络深度的增加，训练变得更加困难==。此外，==现有的方法在处理不同通道的特征时缺乏灵活性==，导致网络的表示能力受限。为了解决这些问题，作者提出了**深度残差通道注意力网络（RCAN）**，通过引入**残差中的残差（RIR）结构**和**通道注意力机制（CA）**，进一步提升超分辨率任务的性能。

## 改进工作方法
### 1 . **残差中的残差结构（RIR）**：
   - RIR 结构由多个残差组（Residual Group, RG）组成，每个残差组包含多个残差块（Residual Block），并通过长跳跃连接（Long Skip Connection, LSC）和短跳跃连接（Short Skip Connection, SSC）进行信息传递。
   - 这种结构==允许低频信息通过多个跳跃连接绕过主网络，使得主网络能够专注于学习高频信息==，从而缓解了深度网络的训练难度。
### 2 . **通道注意力机制（CA）**：
   - 通道注意力机制通过建模通道间的依赖关系，自适应地重新缩放每个通道的特征。具体来说，CA 机制首先通过全局平均池化（Global Average Pooling）生成通道统计量，然后通过一个门控机制（Sigmoid 函数）生成通道权重，最后将这些权重应用于特征图，以增强网络对重要通道的关注。

#### 2 .1. 全局平均池化（Global Average Pooling, GAP）
- 输入特征图 $X$ 的尺寸为 $H \times W \times C$，其中 $H$ 和 $W$ 是空间维度，$C$ 是通道数。
- 对每个通道的特征图进行全局平均池化，生成一个通道统计量 $z$，其尺寸为 $1 \times 1 \times C$。具体公式为：$$Z_c = \frac{1}{H \times W} \sum_{i=1}^H \sum_{j=1}^W x_c (i, j)$$
其中，$x_c (i, j)$ 表示第 $c$ 个通道在位置 $(i, j)$ 处的值。
   
#### 2.2. 通道降维与升维
- 为了捕捉通道间的非线性关系，通道统计量 $z$ 首先通过一个降维卷积层（Conv 1 x 1），将通道数从 $C$ 减少到 $C/r$（$r$ 是降维比例，通常取 16），然后通过 ReLU 激活函数。
- 接着，通过一个升维卷积层（Conv 1 x 1），将通道数从 $C/r$ 恢复到 $C$，并通过 Sigmoid 函数生成通道权重 $s$。具体公式为： $$S = \sigma (W_U \cdot \delta (W_D \cdot z))$$
其中，$W_D$ 和 $W_U$ 分别是降维和升维卷积层的权重，$\delta$ 是 ReLU 激活函数，$\sigma$ 是 Sigmoid 函数。

#### 2 .3. 通道特征重缩放
- 生成的通道权重 $s$ 被应用于输入特征图 $X$，对每个通道的特征图进行自适应缩放。具体公式为：$$\hat{x}_c = s_c \cdot x_c$$
其中，$\hat{x}_c$ 是重缩放后的特征图，$s_c$ 是第 $c$ 个通道的权重。

#### 3 . **残差通道注意力块（RCAB）**：
- RCAB 将通道注意力机制集成到残差块中，进一步增强了网络的表示能力。每个 RCAB 通过两个卷积层提取残差特征，并通过 CA 机制对残差特征进行自适应缩放。

## 效果
1. **定量结果**：
   - 在多个标准数据集（如 Set 5、Set 14、B 100、Urban 100 和 Manga 109）上，RCAN 在 PSNR 和 SSIM 指标上均优于现有的最先进方法。特别是在大尺度（如 8 倍）超分辨率任务中，RCAN 的性能提升更为显著。
2. **定性结果**：
   - 视觉对比结果显示，RCAN 能够更好地恢复图像中的细节，减少模糊和伪影，生成的高分辨率图像更加清晰和真实。
3. **模型复杂度**：
   - 尽管 RCAN 的网络深度非常大（超过 400 层），但其参数量相对较少，且在性能和模型复杂度之间取得了良好的平衡。

## 总结
通过引入**残差中的残差结构**和**通道注意力机制**，RCAN 在图像超分辨率任务中取得了显著的性能提升。实验结果表明，RCAN 不仅在定量指标上优于现有方法，而且在视觉质量上也表现出色，特别是在处理大尺度超分辨率任务时表现尤为突出。

> [!notes] 简单的说就是进一步用深度学习的方法改进了网络，用残差机制让网络能学习到应有的特征，并且用注意力机制（单独学习一个权重网络）提高残差特征的比例。
# 2.2. EDSR/MDSR
## **1. EDSR（Enhanced Deep Super-Resolution Network）**
### **Motivation（动机）**
尽管深度卷积神经网络（CNN）在图像超分辨率任务中取得了显著进展，但作者观察到，==随着网络深度的增加，训练变得更加困难==。此外，==现有的残差网络（如 SRResNet）直接借鉴了用于图像分类的 ResNet 架构，但超分辨率任务属于低层次视觉问题，直接应用 ResNet 可能不是最优的==。为了解决这些问题，作者提出了**增强的深度超分辨率网络（EDSR）**，通过移除不必要的模块（如批归一化层）并扩展模型规模，进一步提升超分辨率任务的性能。
### **改进工作方法**
1. **移除批归一化层（Batch Normalization, BN）**：
   - BN 在超分辨率任务中会限制特征的灵活性，移除 BN 后，网络能够更好地捕捉图像中的细节信息。
   - 移除 BN 还减少了内存占用，使得网络可以扩展到更大的规模。
2. **残差块（Residual Blocks）**：
   - EDSR 包含 32 个残差块，每个残差块由两个卷积层组成，卷积层的滤波器数量为 256，核大小为 3×3。
   - 每个残差块内部使用**跳跃连接（Skip Connection）**，将输入直接添加到输出中，以学习残差信息。
3. **残差缩放（Residual Scaling）**：
   - 在残差块的最后一层卷积后引入一个缩放因子（如 0.1），以稳定训练过程，特别是在使用大量滤波器时。
4. **后上采样策略（Post-upsampling）**：
   - EDSR 采用后上采样策略，即在网络的最后一层进行上采样，减少了计算复杂度。
   - 使用**亚像素卷积层（Sub-pixel Convolution Layer）**实现上采样，通过重排特征图的方式将低分辨率特征图转换为高分辨率图像。
5. **全局跳跃连接（Global Skip Connection）**：
   - 将浅层特征提取的输出直接添加到残差块的最终输出中，以保留低频信息。
### **效果**
1. **定量结果**：
   - 在多个基准数据集（如 Set 5、Set 14、B 100、Urban 100 和 DIV 2 K）上，EDSR 在 PSNR 和 SSIM 指标上均优于现有的最先进方法。
2. **定性结果**：
   - 视觉对比结果显示，EDSR 能够更好地恢复图像中的细节和纹理，生成的高分辨率图像更加清晰和真实。
3. **模型复杂度**：
   - 尽管 EDSR 的网络深度和宽度较大，但由于移除了 BN，其内存占用和计算复杂度相对较低。

## **2. MDSR（Multi-scale Deep Super-Resolution Network）**
### **Motivation（动机）**
大多数现有的超分辨率算法将不同尺度的超分辨率任务视为独立问题，需要为每个尺度训练单独的模型，导致计算资源浪费。为了解决这一问题，作者提出了**多尺度超分辨率系统（MDSR）**，能够在单个模型中处理不同尺度的超分辨率任务。
### **改进工作方法**
1. **共享主干网络（Shared Main Branch）**：
   - MDSR 包含一个共享的主干网络，由 80 个残差块组成，每个残差块的结构与 EDSR 相同。
   - 主干网络用于提取多尺度共享的特征。
2. **尺度特定预处理模块（Scale-specific Pre-processing Modules）**：
   - 每个尺度（如×2、×3、×4）都有一个特定的预处理模块，用于减少输入图像的尺度差异。
   - 每个预处理模块由两个残差块组成，卷积层的核大小为 5×5，以覆盖更大的感受野。
3. **尺度特定上采样模块（Scale-specific Upsampling Modules）**：
   - 每个尺度都有一个特定的上采样模块，用于将低分辨率特征图转换为高分辨率图像。
   - 上采样模块的结构与 EDSR 相同，使用亚像素卷积层实现上采样。
4. **多尺度训练**：
   - 在训练过程中，MDSR 随机选择一个尺度进行训练，只更新与该尺度相关的模块。
   - 通过共享主干网络的大部分参数，显著减少了模型的计算复杂度和参数量。
### **效果**
1. **定量结果**：
   - 在多个基准数据集上，MDSR 在 PSNR 和 SSIM 指标上均优于现有的最先进方法。
2. **定性结果**：
   - 视觉对比结果显示，MDSR 能够更好地恢复图像中的细节和纹理，生成的高分辨率图像更加清晰和真实。
3. **模型复杂度**：
   - MDSR 通过共享主干网络，显著减少了模型的计算复杂度和参数量。

## **3. 训练策略**
### **EDSR 的训练策略**
1. **损失函数**：
   - 使用 L1 损失函数代替传统的 L2 损失函数，实验表明 L1 损失能够带来更好的收敛性和性能。
2. **预训练策略**：
   - 先训练低尺度模型（如×2），然后将其作为高尺度模型（如×3、×4）的初始化，加速训练并提升最终性能。
3. **几何自集成（Geometric Self-ensemble）**：
   - 在测试阶段，通过对输入图像进行几何变换（如翻转和旋转）生成多个增强输入，然后将这些输入的超分辨率结果进行平均，进一步提升模型性能。

### **MDSR 的训练策略**
1. **多尺度训练**：
   - 在训练过程中，随机选择一个尺度进行训练，只更新与该尺度相关的模块。
2. **参数共享**：
   - 通过共享主干网络的大部分参数，减少了模型的计算复杂度和参数量。

> [!notes] 直接堆积了一堆残差块；引入了用网络上采样（就是那个“亚像素卷积层”），然后微调了很多炼丹的细节和操作

# 2.3.  MSRN
> [!notes] 实在难以评价这种纯炼丹论文了，感觉整个叙述逻辑都虚虚的......


