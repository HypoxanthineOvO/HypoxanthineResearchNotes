# 6.2（周一）
## 1. ML PJ
在 NGP 上实现了评估，多跑一些数据，要准备开始写报告了
# 6.3（周二）
## 1. ML PJ
### 实现 MAT
看看到底多久会收敛（？）主要是还有学习率的影响
### 数据分析
- NGP 一直在调整 color MLP，其他的模块在摆大烂（）
- 感觉可以考虑根据这个结果去验证，哈希表一下就学好了
- 下一步除了用他的 MAT 之外也可以考虑根据这个结论去调整不同模块的学习率
### 后续论文主要框架设计
1. 前两章做推导和证明
2. 介绍他的策略
3. 介绍神经渲染任务，解读为编码和存储任务
4. 提出我们对神经渲染的想法，然后探索对密度编码和对颜色编码的难度，以及容量之类的信息（？）
5. 然后放对比实验，主要包括：
	1. 不同配置下 NGP 的效果、Lambda Max 等的分布
	2. 加上 MAT 策略之后的优化比例
	3. NGP vs NeRF，对比神经网络 x Hash Table

## 2. 毕设视频和海报
毕设的简单介绍：
1. 我的个人信息
2. 毕设介绍：神经渲染的重要性（30 字），系统的分析了其中的瓶颈（50 字），提出了针对最大瓶颈的方案并做了硬件实现，实现了几百倍的能效比和实时渲染效果

视频部分：
- 10 s 个人介绍
- 40 s：NeRF 的兴起，诸多方法，我们进行了系统的对比
- 20 s：图表的展示，主要通过图表去分析瓶颈
- 30 s：Ray Casting 的动画和投影的动画讲解
- 30 s：Hash Encoding 的动画
- 20 s：结果展示
- 30 s：用 FPGA 抠图+视频展示我们的渲染效果

# 6.4（周三）
## 1. ML Project
今天要实现逐层哈希表的分析

## 2. 毕设视频
### 2.1. 几句话介绍神经渲染
神经渲染结合深度学习与计算机图形学，通过神经网络学习场景表示（如NeRF的辐射场建模），实现三维重建和新视角合成等高任务。
神经渲染的主要管线一般可以分为：12345
### 2.2. 性能分析
在 Jetson Xavier NX，RTX 3050 Laptop 和 RTX 3090 Ti 上进行 Time Breakdown，可以发现其中最耗时的两个阶段是 1 和 2
### 2.3. 主要瓶颈探索
#### 2.3.1. 光线投射
其主要瓶颈是由于采样的效率过低
#### 2.3.2. 哈希编码
访存墙

### 2.4. 光线投射系统
迭代附着法+投影采样
### 2.5. 哈希编码引擎

# 6.5（周四）

# 6.6（周五）

